{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym, gym_unbalanced_disk, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(theta):\n",
    "    return (theta+np.pi)%(2*np.pi) - np.pi # map to [-pi,pi]\n",
    "\n",
    "class Discretize(gym.Wrapper): # only discrete action\n",
    "    def __init__(self,env,num_act=7):\n",
    "        super(Discretize,self).__init__(env) #sets self.env , call function from father calss\n",
    "        self.num_act = num_act\n",
    "        self.action_space = gym.spaces.Discrete(self.num_act)\n",
    "        self.alow, self.ahigh = env.action_space.low, env.action_space.high\n",
    "        # action discrete list.--no need! since we map action from index like to (-3,3)\n",
    "        # self.stepsize = (self.ahigh-self.alow)/self.num_act\n",
    "        # self.act_values_list = np.arange(self.alow,self.ahigh,self.stepsize)\n",
    "        \n",
    "    def step(self,action):\n",
    "        action = self.discretize_act(action)\n",
    "        obs,_,done,info = self.env.step(action)\n",
    "        # velocity from (-infi,infi) to (-pi,pi)\n",
    "        obs[0] = normalize(obs[0])\n",
    "        reward = self.reward_fc(obs,action=action)\n",
    "\n",
    "        return np.array(obs),reward,done,info\n",
    "\n",
    "    def discretize_act(self,action): ##!!! action input is from 0 1 2... num-1;  output is -3,...0,...3\n",
    "        # stepsize = (self.ahigh-self.alow)/self.num_act\n",
    "        # values_list = np.arange(self.alow,self.ahigh,stepsize)\n",
    "        # out = values_list[np.abs(values_list-action).argmin()] \n",
    "        step_size = (self.ahigh-self.alow)/(self.num_act-1)\n",
    "        action = action*step_size +self.alow\n",
    "        return action\n",
    "    \n",
    "    def reward_fc(self,obs,action):\n",
    "        theta = normalize(obs[0]) # already mapped so [-pi,pi]\n",
    "        omega = obs[1]\n",
    "        # reward_vel = omega/40 * np.exp(-abs(theta)) # /40 to reduce -> (0,1) \n",
    "        reward_th =  np.exp(- (abs(theta)-np.pi)**2/(2*(np.pi/10)**2)) # **2 so no abs here!\n",
    "       \n",
    "        if abs(theta)>3 and omega<0.1 :\n",
    "            if abs(action)<=1:\n",
    "                reward =    2*reward_th +5*(3-abs(action)) +2 # 2*reward_vel +10\n",
    "            elif abs(action)>2:\n",
    "                reward =  2*reward_th - 2* abs(action)\n",
    "            else:\n",
    "                reward =    2*reward_th + 1*(3-abs(action)) + 2\n",
    "        elif abs(theta)<1/3*np.pi and omega<0.5:\n",
    "            reward =    2*reward_th + abs(action) -3  # 2*reward_vel -1 \n",
    "        else:\n",
    "            reward =     2*reward_th +2  # 2*reward_vel\n",
    "\n",
    "        # alpha, beta, gamma = 100, 0.05, 0.5\n",
    "        # reward = alpha*theta**2 - beta*omega**2 - gamma*action**2\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def reset(self):\n",
    "        return np.array(self.env.reset())\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, env,num_hidden_cri=40, num_hidden_act=40):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        num_inputs = env.observation_space.shape[0] # 2 elements\n",
    "        num_acts = env.action_space.n # discretized action space\n",
    "\n",
    "        # define critic layers \n",
    "        self.cri_linear1 = nn.Linear(num_inputs,num_hidden_cri)\n",
    "        self.cri_linear2 = nn.Linear(num_hidden_cri,1)\n",
    "\n",
    "        # define actor layers\n",
    "        self.act_linear1 =nn.Linear(num_inputs,num_hidden_act)\n",
    "        self.act_linear2 = nn.Linear(num_hidden_act,num_acts)\n",
    "        #self.softmax = nn.Softmax(dim=-1) \n",
    "\n",
    "    def forward(self,state): # (batch, obs)\n",
    "        return self.critic(state),self.actor(state)\n",
    "\n",
    "    def critic(self,state):\n",
    "        a=torch.tanh(self.cri_linear1(state))\n",
    "        out = self.cri_linear2(a)[:,0] # N*1\n",
    "        return out\n",
    "    \n",
    "    def actor(self,state,return_logp=False):\n",
    "        a = torch.tanh(self.act_linear1(state))\n",
    "        a = self.act_linear2(a)\n",
    "        a = a - torch.max(a,dim=1,keepdim=True)[0] # for each sample, find max value action, -max\n",
    "        #p_a = self.softmax(a) # probability\n",
    "\n",
    "        logp = a - torch.log(torch.sum(torch.exp(a),dim=1,keepdim=True)) #log of the softmax, so called log_softmax, is not log(softmax)!\n",
    "\n",
    "        if return_logp ==False:\n",
    "            return torch.exp(logp) # (num_acts,1)\n",
    "        \n",
    "        \n",
    "        if return_logp ==True:\n",
    "\n",
    "            return logp\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROLLOUT interact with env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(actor_crit, env, N_rollout=10_000):\n",
    "    #save the following (use .append)\n",
    "    Start_state = [] # holding an array of (x_t)\n",
    "    Actions = []     # holding an array of (u_t)\n",
    "    Rewards = []     # holding an array of (r_{t+1})\n",
    "    End_state = []   # holding an array of (x_{t+1})\n",
    "    Terminal = []    # holding an array of (terminal_{t+1})\n",
    "    # actor as policy pi\n",
    "    pi = lambda input: actor_crit.actor(torch.tensor(input[None,:],dtype=torch.float32))[0].numpy()\n",
    "    with torch.no_grad():\n",
    "        obs = env.reset()\n",
    "        for i in range(N_rollout):\n",
    "            # based on probability, randomly choose action # based actor results, sample index!?\n",
    "            action = np.random.choice(a=env.action_space.n,p=pi(obs)) #b=) env.act_values_list\n",
    "\n",
    "            Start_state.append(obs)\n",
    "            Actions.append(action)\n",
    "\n",
    "            obs_next, reward, done, info = env.step(action)\n",
    "\n",
    "            terminal = done and not info.get('TimeLimit.truncated', False)\n",
    "\n",
    "            Terminal.append(terminal)\n",
    "            Rewards.append(reward)\n",
    "            End_state.append(obs_next)\n",
    "\n",
    "            if done:\n",
    "                obs = env.reset()\n",
    "            else:\n",
    "                obs = obs_next\n",
    "\n",
    "    #error checking:\n",
    "    assert len(Start_state)==len(Actions)==len(Rewards)==len(End_state)==len(Terminal), f'error in lengths: {len(Start_state)}=={len(Actions)}=={len(Rewards)}=={len(End_state)}=={len(Terminal)}'\n",
    "    return np.array(Start_state), np.array(Actions), np.array(Rewards), np.array(End_state), np.array(Terminal).astype(int)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_actor(actor_critic, env):\n",
    "    pi = lambda x: actor_critic.actor(torch.tensor(x[None,:],dtype=torch.float32))[0].numpy()\n",
    "    with torch.no_grad():\n",
    "        rewards_acc = 0\n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "            action = np.argmax(pi(obs)) #b=)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            rewards_acc += reward\n",
    "            if done:\n",
    "                return rewards_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A2C_train(actor_critic, optimizer, env, N_iter=21,N_rollout=20000,N_epochs=10, batch_size=32,N_evals=10,\\\n",
    "              alpha_actor=0.5,alpha_entropy=0.5,gamma=0.98):\n",
    "    best = -float('inf')\n",
    "    #torch.save(actor_critic.state_dict(),'A2C-Best.pth')\n",
    "\n",
    "    try:\n",
    "        for iteration in range(N_iter):\n",
    "            print(f'Rollout iteration {iteration+1}')\n",
    "            # rollout to get trajectory record\n",
    "            Start_state, Actions, Rewards, End_state, Terminal = rollout(actor_critic, env, N_rollout=N_rollout)\n",
    "            # data \n",
    "            Start_state = torch.tensor(Start_state,dtype=torch.float32)\n",
    "            Rewards = torch.tensor(Rewards,dtype=torch.float32)\n",
    "            End_state =torch.tensor(End_state,dtype=torch.float32)\n",
    "            Terminal = torch.tensor(Terminal,dtype=torch.float32)\n",
    "            Actions = Actions.astype(int)\n",
    "\n",
    "            print('Starting training on rollout information...')\n",
    "            for epoch in range(N_epochs):\n",
    "                for i in range(batch_size,len(Start_state)+1,batch_size):\n",
    "                    Start_state_batch, Actions_batch, Rewards_batch, End_state_batch, Terminal_batch = \\\n",
    "                    [d[i-batch_size:i] for d in [Start_state, Actions, Rewards, End_state, Terminal]]\n",
    "\n",
    "                    #Advantage:\n",
    "                    Vnow = actor_critic.critic(Start_state_batch) \n",
    "                    Vnext = actor_critic.critic(End_state_batch) \n",
    "                    A = Rewards_batch + gamma*Vnext*(1-Terminal_batch) - Vnow \n",
    "\n",
    "                    # convert from action to index.  # Now action is 0 1 2.. index value，convert in step function=>（-3，3）\n",
    "                    ##### \n",
    "                    \n",
    "                    action_index = np.stack((np.arange(batch_size),Actions_batch),axis=0)\n",
    "                    logp = actor_critic.actor(Start_state_batch,return_logp=True)# return is 【N——batch，num_action】 \n",
    "                    logp_cur = logp[action_index] # do slice，dim0 batch，dim1 use action value ，to get probability\n",
    "                    p = torch.exp(logp) \n",
    "\n",
    "                    L_value_function = torch.mean(A**2) \n",
    "                    L_policy = -(A.detach()*logp_cur).mean() #detach A, the gradient should only to through logp\n",
    "                    L_entropy = -torch.mean((-p*logp),0).sum() \n",
    "\n",
    "                    Loss = L_value_function + alpha_actor*L_policy + alpha_entropy*L_entropy \n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    Loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                print(f'logp{p[0]} logp{logp.shape}')\n",
    "\n",
    "                score = np.mean([eval_actor(actor_critic, env) for i in range(N_evals)])\n",
    "                \n",
    "                print(f'iteration={iteration+1} epoch={epoch+1} Average Reward per episode:',score)\n",
    "                print(f'\\t Value loss:  {L_value_function.item(): .4f}')\n",
    "                print(f'\\t Policy loss: {L_policy.item(): .4f}')\n",
    "                print(f'\\t Entropy:     {-L_entropy.item(): .4f}')\n",
    "\n",
    "                if score>best:\n",
    "                    best = score\n",
    "                    print(f'################################# \\n new best {best: .4f} saving actor-crit... \\n#################################')\n",
    "                    torch.save(actor_critic.state_dict(),'A2C-Best.pth')\n",
    "            print('loading best result')\n",
    "            actor_critic.load_state_dict(torch.load('A2C-Best.pth'))\n",
    "    finally: # this will always run even when using the a KeyBoard Interrupt.\n",
    "        print('loading best result')\n",
    "        actor_critic.load_state_dict(torch.load('A2C-Best.pth'))\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(actor_critic,env):\n",
    "    pi = lambda x: actor_critic.actor(torch.tensor(x[None,:],dtype=torch.float32))[0].numpy()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            obs = env.reset()\n",
    "            env.render()\n",
    "            time.sleep(1)\n",
    "            while True:\n",
    "                action = np.argmax(pi(obs)) \n",
    "                obs, reward, done, info = env.step(action)\n",
    "                print(obs, reward, done, info)\n",
    "                time.sleep(1/60)\n",
    "                env.render()\n",
    "                if done:\n",
    "                    time.sleep(0.5)\n",
    "                    break\n",
    "        finally: #this will always run even when an error occurs\n",
    "            env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollout iteration 1\n",
      "Starting training on rollout information...\n",
      "logptensor([2.4622e-01, 1.6910e-01, 1.9184e-02, 1.5379e-03, 1.5028e-04, 5.1533e-04,\n",
      "        3.6081e-04, 3.3580e-04, 1.2432e-01, 1.3058e-01, 3.0770e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=1 epoch=1 Average Reward per episode: 40.00000000007157\n",
      "\t Value loss:   2.8956\n",
      "\t Policy loss: -4.7647\n",
      "\t Entropy:      1.7107\n",
      "################################# \n",
      " new best  40.0000 saving actor-crit... \n",
      "#################################\n",
      "logptensor([3.8603e-01, 1.1213e-01, 2.4504e-09, 7.0497e-11, 1.9847e-13, 6.6202e-12,\n",
      "        1.7860e-12, 1.1729e-12, 1.9670e-05, 2.7581e-04, 5.0154e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=1 epoch=2 Average Reward per episode: 1965.0033367696983\n",
      "\t Value loss:   1.9375\n",
      "\t Policy loss: -16.0626\n",
      "\t Entropy:      1.3302\n",
      "################################# \n",
      " new best  1965.0033 saving actor-crit... \n",
      "#################################\n",
      "logptensor([4.3668e-01, 7.8384e-05, 8.4628e-19, 1.0346e-20, 6.0874e-25, 2.0050e-22,\n",
      "        1.8504e-23, 1.2018e-23, 1.6358e-12, 2.2890e-10, 5.6324e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=1 epoch=3 Average Reward per episode: 249.4000000000001\n",
      "\t Value loss:   1.0138\n",
      "\t Policy loss: -17.3035\n",
      "\t Entropy:      1.1627\n",
      "logptensor([4.4256e-01, 1.2730e-08, 1.0151e-25, 1.7776e-29, 2.0031e-35, 5.4267e-32,\n",
      "        1.7641e-33, 2.9661e-33, 1.8491e-18, 2.6576e-15, 5.5744e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=1 epoch=4 Average Reward per episode: 35.40000000013686\n",
      "\t Value loss:   0.7469\n",
      "\t Policy loss: -13.1933\n",
      "\t Entropy:      1.1088\n",
      "logptensor([4.4526e-01, 4.2998e-11, 2.2371e-30, 3.4606e-37, 7.0065e-45, 1.3699e-40,\n",
      "        1.6844e-42, 1.3162e-41, 2.8758e-23, 4.9875e-19, 5.5474e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=1 epoch=5 Average Reward per episode: 299.4\n",
      "\t Value loss:   0.7054\n",
      "\t Policy loss: -13.2179\n",
      "\t Entropy:      1.0202\n",
      "loading best result\n",
      "Rollout iteration 2\n",
      "Starting training on rollout information...\n",
      "logptensor([4.3339e-01, 1.9927e-01, 1.5817e-13, 4.1867e-16, 2.1401e-19, 1.3216e-17,\n",
      "        2.3677e-18, 1.1904e-18, 3.0554e-08, 1.9251e-06, 3.6734e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=2 epoch=1 Average Reward per episode: 307.59704501445515\n",
      "\t Value loss:   1.5731\n",
      "\t Policy loss:  1.1645\n",
      "\t Entropy:      1.2004\n",
      "logptensor([4.3362e-01, 1.8957e-01, 1.2053e-13, 4.6500e-16, 4.3553e-19, 1.8570e-17,\n",
      "        4.2939e-18, 2.1704e-18, 5.1212e-08, 2.7851e-06, 3.7681e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=2 epoch=2 Average Reward per episode: 122.40000006787935\n",
      "\t Value loss:   1.5177\n",
      "\t Policy loss:  1.1459\n",
      "\t Entropy:      1.2436\n",
      "logptensor([4.3175e-01, 1.9156e-01, 9.3535e-14, 7.8026e-16, 7.4762e-19, 3.0167e-17,\n",
      "        7.7647e-18, 3.8027e-18, 8.1715e-08, 3.9919e-06, 3.7669e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=2 epoch=3 Average Reward per episode: 404.0000000000009\n",
      "\t Value loss:   1.4890\n",
      "\t Policy loss:  1.1360\n",
      "\t Entropy:      1.2763\n",
      "logptensor([4.3092e-01, 1.9403e-01, 8.9326e-14, 1.5411e-15, 1.2378e-18, 5.2250e-17,\n",
      "        1.3844e-17, 6.6240e-18, 1.2722e-07, 5.6932e-06, 3.7504e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=2 epoch=4 Average Reward per episode: 157.600000000162\n",
      "\t Value loss:   1.4667\n",
      "\t Policy loss:  1.1298\n",
      "\t Entropy:      1.3058\n",
      "logptensor([4.3042e-01, 1.9621e-01, 1.1829e-13, 3.1735e-15, 2.1049e-18, 9.6067e-17,\n",
      "        2.5228e-17, 1.2027e-17, 2.0179e-07, 8.2123e-06, 3.7336e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=2 epoch=5 Average Reward per episode: 268.0000000001036\n",
      "\t Value loss:   1.4448\n",
      "\t Policy loss:  1.1277\n",
      "\t Entropy:      1.3362\n",
      "loading best result\n",
      "Rollout iteration 3\n",
      "Starting training on rollout information...\n",
      "logptensor([0.0091, 0.0082, 0.1178, 0.1366, 0.1085, 0.0993, 0.1108, 0.1450, 0.1738,\n",
      "        0.0838, 0.0071], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=3 epoch=1 Average Reward per episode: 30.000000000225846\n",
      "\t Value loss:   0.9717\n",
      "\t Policy loss:  1.1425\n",
      "\t Entropy:      1.3468\n",
      "logptensor([0.0146, 0.0137, 0.1178, 0.0994, 0.1214, 0.0888, 0.1086, 0.1432, 0.1843,\n",
      "        0.0961, 0.0121], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=3 epoch=2 Average Reward per episode: 28.20000000020849\n",
      "\t Value loss:   0.9505\n",
      "\t Policy loss:  1.1470\n",
      "\t Entropy:      1.3764\n",
      "logptensor([0.0225, 0.0213, 0.0999, 0.0884, 0.1289, 0.0833, 0.1044, 0.1396, 0.1890,\n",
      "        0.1027, 0.0199], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=3 epoch=3 Average Reward per episode: 100.8000000219214\n",
      "\t Value loss:   0.9352\n",
      "\t Policy loss:  1.1505\n",
      "\t Entropy:      1.4029\n",
      "logptensor([0.0340, 0.0317, 0.0793, 0.0843, 0.1419, 0.0788, 0.0983, 0.1356, 0.1799,\n",
      "        0.1045, 0.0318], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=3 epoch=4 Average Reward per episode: 108.00000000027826\n",
      "\t Value loss:   0.9210\n",
      "\t Policy loss:  1.1544\n",
      "\t Entropy:      1.4326\n",
      "logptensor([0.0439, 0.0398, 0.0729, 0.0824, 0.1703, 0.0773, 0.0935, 0.1368, 0.1497,\n",
      "        0.0902, 0.0433], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=3 epoch=5 Average Reward per episode: 360.00000000020987\n",
      "\t Value loss:   0.9059\n",
      "\t Policy loss:  1.1584\n",
      "\t Entropy:      1.4608\n",
      "loading best result\n",
      "Rollout iteration 4\n",
      "Starting training on rollout information...\n",
      "logptensor([3.9639e-01, 1.9143e-01, 3.1731e-13, 2.0898e-15, 5.5099e-18, 1.0052e-16,\n",
      "        2.6583e-17, 1.5240e-17, 9.5030e-08, 9.9336e-06, 4.1217e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=4 epoch=1 Average Reward per episode: 418.0010809014302\n",
      "\t Value loss:   2.3586\n",
      "\t Policy loss:  1.4768\n",
      "\t Entropy:      1.2510\n",
      "logptensor([3.8829e-01, 1.8733e-01, 4.3136e-13, 6.6136e-15, 3.0347e-17, 5.1054e-16,\n",
      "        1.8108e-16, 7.9375e-17, 1.6418e-07, 2.2504e-05, 4.2436e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=4 epoch=2 Average Reward per episode: 404.0000011272557\n",
      "\t Value loss:   2.2476\n",
      "\t Policy loss:  1.4656\n",
      "\t Entropy:      1.2945\n",
      "logptensor([3.8094e-01, 1.7680e-01, 2.0774e-12, 8.5128e-14, 4.8013e-16, 8.3029e-15,\n",
      "        3.1344e-15, 1.2125e-15, 4.2890e-07, 6.1129e-05, 4.4220e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=4 epoch=3 Average Reward per episode: 386.60000000000383\n",
      "\t Value loss:   2.1839\n",
      "\t Policy loss:  1.4545\n",
      "\t Entropy:      1.3310\n",
      "logptensor([3.7834e-01, 1.6524e-01, 1.2785e-10, 1.0250e-11, 8.5745e-14, 1.3235e-12,\n",
      "        5.4200e-13, 2.0336e-13, 3.6949e-06, 2.8087e-04, 4.5614e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=4 epoch=4 Average Reward per episode: 332.00000000006224\n",
      "\t Value loss:   2.1376\n",
      "\t Policy loss:  1.4548\n",
      "\t Entropy:      1.3826\n",
      "logptensor([3.8277e-01, 1.6757e-01, 2.4215e-08, 2.8607e-09, 3.7245e-11, 4.5202e-10,\n",
      "        2.0442e-10, 7.6160e-11, 7.1055e-05, 2.2498e-03, 4.4734e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=4 epoch=5 Average Reward per episode: 323.60000000016754\n",
      "\t Value loss:   2.0986\n",
      "\t Policy loss:  1.4766\n",
      "\t Entropy:      1.4486\n",
      "loading best result\n",
      "Rollout iteration 5\n",
      "Starting training on rollout information...\n",
      "logptensor([0.1893, 0.1785, 0.0360, 0.0240, 0.0100, 0.0168, 0.0168, 0.0079, 0.0995,\n",
      "        0.1075, 0.3138], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=5 epoch=1 Average Reward per episode: 523.6453982815699\n",
      "\t Value loss:   2.9137\n",
      "\t Policy loss:  1.7374\n",
      "\t Entropy:      1.3519\n",
      "logptensor([0.1093, 0.0968, 0.1039, 0.0740, 0.0299, 0.0424, 0.0495, 0.0212, 0.1170,\n",
      "        0.1209, 0.2350], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=5 epoch=2 Average Reward per episode: 458.01538144199174\n",
      "\t Value loss:   2.8929\n",
      "\t Policy loss:  1.6391\n",
      "\t Entropy:      1.4068\n",
      "logptensor([0.0647, 0.0570, 0.1708, 0.1099, 0.0434, 0.0582, 0.0752, 0.0303, 0.1126,\n",
      "        0.1153, 0.1627], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=5 epoch=3 Average Reward per episode: 353.10482274380985\n",
      "\t Value loss:   2.8882\n",
      "\t Policy loss:  1.6126\n",
      "\t Entropy:      1.4575\n",
      "logptensor([0.0476, 0.0437, 0.1941, 0.1226, 0.0448, 0.0608, 0.0835, 0.0314, 0.1158,\n",
      "        0.1213, 0.1345], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=5 epoch=4 Average Reward per episode: 457.8389479607148\n",
      "\t Value loss:   2.8771\n",
      "\t Policy loss:  1.6323\n",
      "\t Entropy:      1.5184\n",
      "logptensor([0.0358, 0.0346, 0.1960, 0.1318, 0.0441, 0.0619, 0.0910, 0.0311, 0.1218,\n",
      "        0.1383, 0.1135], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=5 epoch=5 Average Reward per episode: 381.733460949637\n",
      "\t Value loss:   2.8573\n",
      "\t Policy loss:  1.6605\n",
      "\t Entropy:      1.5766\n",
      "loading best result\n",
      "Rollout iteration 6\n",
      "Starting training on rollout information...\n",
      "logptensor([4.0402e-01, 1.9291e-01, 1.9896e-12, 3.2611e-14, 9.6000e-17, 1.2888e-15,\n",
      "        6.6937e-16, 1.9375e-16, 2.9675e-07, 1.5151e-05, 4.0305e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=6 epoch=1 Average Reward per episode: 26.00000000015222\n",
      "\t Value loss:   0.9616\n",
      "\t Policy loss:  1.2624\n",
      "\t Entropy:      1.4765\n",
      "logptensor([3.9603e-01, 1.9052e-01, 8.3762e-12, 3.4872e-13, 1.3811e-15, 1.8450e-14,\n",
      "        1.1914e-14, 3.0610e-15, 9.5484e-07, 3.7862e-05, 4.1341e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=6 epoch=2 Average Reward per episode: 380.0000000092846\n",
      "\t Value loss:   0.9296\n",
      "\t Policy loss:  1.2591\n",
      "\t Entropy:      1.5411\n",
      "logptensor([3.9886e-01, 1.9149e-01, 1.3698e-10, 8.8184e-12, 3.8764e-14, 5.3677e-13,\n",
      "        3.6206e-13, 8.9290e-14, 5.0618e-06, 1.2240e-04, 4.0953e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=6 epoch=3 Average Reward per episode: 312.80000000023574\n",
      "\t Value loss:   0.9088\n",
      "\t Policy loss:  1.3211\n",
      "\t Entropy:      1.6071\n",
      "logptensor([3.9780e-01, 1.9399e-01, 1.6872e-09, 1.2533e-10, 6.5205e-13, 8.7707e-12,\n",
      "        5.9305e-12, 1.4409e-12, 2.6859e-05, 4.7216e-04, 4.0771e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=6 epoch=4 Average Reward per episode: 317.2000000014914\n",
      "\t Value loss:   0.8956\n",
      "\t Policy loss:  1.3732\n",
      "\t Entropy:      1.6483\n",
      "logptensor([3.9324e-01, 1.9020e-01, 1.1666e-08, 9.3554e-10, 5.7669e-12, 7.5917e-11,\n",
      "        5.2459e-11, 1.2213e-11, 9.9385e-05, 1.8692e-03, 4.1459e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=6 epoch=5 Average Reward per episode: 323.60000000004595\n",
      "\t Value loss:   0.8851\n",
      "\t Policy loss:  1.3833\n",
      "\t Entropy:      1.6681\n",
      "loading best result\n",
      "Rollout iteration 7\n",
      "Starting training on rollout information...\n",
      "logptensor([0.0236, 0.0189, 0.1346, 0.0738, 0.1026, 0.1080, 0.1139, 0.1009, 0.2085,\n",
      "        0.0993, 0.0157], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=7 epoch=1 Average Reward per episode: 306.0000000000001\n",
      "\t Value loss:   1.4776\n",
      "\t Policy loss:  1.4125\n",
      "\t Entropy:      1.4404\n",
      "logptensor([0.0336, 0.0262, 0.0873, 0.0613, 0.1167, 0.1172, 0.1266, 0.1046, 0.1982,\n",
      "        0.1063, 0.0221], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=7 epoch=2 Average Reward per episode: 251.80000000000004\n",
      "\t Value loss:   1.4522\n",
      "\t Policy loss:  1.4222\n",
      "\t Entropy:      1.4763\n",
      "logptensor([0.0457, 0.0344, 0.0670, 0.0609, 0.1215, 0.1189, 0.1286, 0.1046, 0.1763,\n",
      "        0.1136, 0.0287], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=7 epoch=3 Average Reward per episode: 253.80000000000004\n",
      "\t Value loss:   1.4354\n",
      "\t Policy loss:  1.4489\n",
      "\t Entropy:      1.5172\n",
      "logptensor([0.0580, 0.0420, 0.0600, 0.0605, 0.1258, 0.1206, 0.1289, 0.1071, 0.1509,\n",
      "        0.1128, 0.0335], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=7 epoch=4 Average Reward per episode: 306.0\n",
      "\t Value loss:   1.4219\n",
      "\t Policy loss:  1.4785\n",
      "\t Entropy:      1.5595\n",
      "logptensor([0.0640, 0.0455, 0.0587, 0.0614, 0.1326, 0.1217, 0.1285, 0.1091, 0.1377,\n",
      "        0.1055, 0.0354], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=7 epoch=5 Average Reward per episode: 364.0\n",
      "\t Value loss:   1.4085\n",
      "\t Policy loss:  1.4957\n",
      "\t Entropy:      1.5936\n",
      "loading best result\n",
      "Rollout iteration 8\n",
      "Starting training on rollout information...\n",
      "logptensor([3.6656e-01, 1.9767e-01, 8.9399e-13, 1.2665e-14, 3.9613e-17, 5.4439e-16,\n",
      "        2.2996e-16, 8.1803e-17, 1.5939e-07, 1.5148e-05, 4.3576e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=8 epoch=1 Average Reward per episode: 56.000000000208544\n",
      "\t Value loss:   1.8312\n",
      "\t Policy loss:  1.3414\n",
      "\t Entropy:      1.3756\n",
      "logptensor([3.3507e-01, 1.7368e-01, 1.4962e-11, 7.7189e-13, 4.8894e-15, 6.8711e-14,\n",
      "        3.4439e-14, 1.0320e-14, 8.1188e-07, 6.3196e-05, 4.9119e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=8 epoch=2 Average Reward per episode: 31.800000000216578\n",
      "\t Value loss:   1.7687\n",
      "\t Policy loss:  1.3591\n",
      "\t Entropy:      1.4552\n",
      "logptensor([3.4535e-01, 1.6439e-01, 2.6506e-09, 3.2936e-10, 3.3867e-12, 4.0631e-11,\n",
      "        2.4115e-11, 6.9894e-12, 1.1261e-05, 3.9724e-04, 4.8986e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=8 epoch=3 Average Reward per episode: 31.60000000021349\n",
      "\t Value loss:   1.7460\n",
      "\t Policy loss:  1.3894\n",
      "\t Entropy:      1.5255\n",
      "logptensor([3.6111e-01, 1.8011e-01, 2.2817e-07, 4.2140e-08, 6.3547e-10, 6.2175e-09,\n",
      "        4.3506e-09, 1.3013e-09, 1.3878e-04, 2.3709e-03, 4.5627e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=8 epoch=4 Average Reward per episode: 28.000000000211276\n",
      "\t Value loss:   1.7341\n",
      "\t Policy loss:  1.4160\n",
      "\t Entropy:      1.5801\n",
      "logptensor([3.6524e-01, 1.8216e-01, 2.8907e-06, 6.7816e-07, 1.2604e-08, 1.1581e-07,\n",
      "        8.6231e-08, 2.6896e-08, 6.5877e-04, 1.0488e-02, 4.4145e-01],\n",
      "       grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=8 epoch=5 Average Reward per episode: 34.00000000024601\n",
      "\t Value loss:   1.7236\n",
      "\t Policy loss:  1.4376\n",
      "\t Entropy:      1.6255\n",
      "loading best result\n",
      "Rollout iteration 9\n",
      "Starting training on rollout information...\n",
      "logptensor([0.0018, 0.0014, 0.0662, 0.1564, 0.2302, 0.1351, 0.1970, 0.1550, 0.0384,\n",
      "        0.0176, 0.0008], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=9 epoch=1 Average Reward per episode: 44.00000000018388\n",
      "\t Value loss:   1.2799\n",
      "\t Policy loss:  1.1368\n",
      "\t Entropy:      1.2768\n",
      "logptensor([0.0111, 0.0082, 0.0735, 0.1078, 0.2307, 0.1109, 0.1799, 0.1366, 0.0825,\n",
      "        0.0534, 0.0053], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=9 epoch=2 Average Reward per episode: 44.000000000183405\n",
      "\t Value loss:   1.2291\n",
      "\t Policy loss:  1.1385\n",
      "\t Entropy:      1.3379\n",
      "logptensor([0.0344, 0.0221, 0.0519, 0.0896, 0.2151, 0.0914, 0.1561, 0.1156, 0.1083,\n",
      "        0.0988, 0.0167], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=9 epoch=3 Average Reward per episode: 248.00000000000045\n",
      "\t Value loss:   1.1910\n",
      "\t Policy loss:  1.1517\n",
      "\t Entropy:      1.4160\n",
      "logptensor([0.0578, 0.0327, 0.0460, 0.0830, 0.2135, 0.0828, 0.1430, 0.1077, 0.0984,\n",
      "        0.1080, 0.0271], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=9 epoch=4 Average Reward per episode: 324.60000000000053\n",
      "\t Value loss:   1.1593\n",
      "\t Policy loss:  1.1720\n",
      "\t Entropy:      1.4908\n",
      "logptensor([0.0675, 0.0389, 0.0455, 0.0828, 0.2197, 0.0814, 0.1412, 0.1090, 0.0874,\n",
      "        0.0946, 0.0320], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=9 epoch=5 Average Reward per episode: 342.00000000000017\n",
      "\t Value loss:   1.1314\n",
      "\t Policy loss:  1.1653\n",
      "\t Entropy:      1.5190\n",
      "loading best result\n",
      "Rollout iteration 10\n",
      "Starting training on rollout information...\n",
      "logptensor([0.0008, 0.0010, 0.0734, 0.1368, 0.2537, 0.1017, 0.1841, 0.1929, 0.0433,\n",
      "        0.0119, 0.0005], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=10 epoch=1 Average Reward per episode: 30.00000000026602\n",
      "\t Value loss:   1.0111\n",
      "\t Policy loss:  1.2546\n",
      "\t Entropy:      1.3852\n",
      "logptensor([0.0096, 0.0120, 0.0804, 0.1022, 0.2204, 0.0769, 0.1562, 0.1603, 0.1210,\n",
      "        0.0538, 0.0072], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=10 epoch=2 Average Reward per episode: 30.000000000266475\n",
      "\t Value loss:   0.9936\n",
      "\t Policy loss:  1.2665\n",
      "\t Entropy:      1.4448\n",
      "logptensor([0.0298, 0.0353, 0.0599, 0.0905, 0.1835, 0.0650, 0.1316, 0.1387, 0.1447,\n",
      "        0.0990, 0.0219], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=10 epoch=3 Average Reward per episode: 30.00000000026669\n",
      "\t Value loss:   0.9841\n",
      "\t Policy loss:  1.2984\n",
      "\t Entropy:      1.5129\n",
      "logptensor([0.0511, 0.0486, 0.0532, 0.0867, 0.1731, 0.0609, 0.1254, 0.1330, 0.1331,\n",
      "        0.1010, 0.0339], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=10 epoch=4 Average Reward per episode: 59.20000000022812\n",
      "\t Value loss:   0.9776\n",
      "\t Policy loss:  1.3079\n",
      "\t Entropy:      1.5573\n",
      "logptensor([0.0637, 0.0524, 0.0536, 0.0897, 0.1765, 0.0628, 0.1301, 0.1405, 0.1199,\n",
      "        0.0705, 0.0403], grad_fn=<SelectBackward0>) logptorch.Size([32, 11])\n",
      "iteration=10 epoch=5 Average Reward per episode: 35.60000000014598\n",
      "\t Value loss:   0.9713\n",
      "\t Policy loss:  1.3205\n",
      "\t Entropy:      1.6313\n",
      "loading best result\n",
      "loading best result\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02181725 1.90248623] 2.0 False {}\n",
      "[0.0807325 2.7741735] 2.0 False {}\n",
      "[0.14972908 2.5794117 ] 2.0 False {}\n",
      "[0.22029066 3.01347671] 2.0 False {}\n",
      "[0.28840246 2.38124783] 2.0 False {}\n",
      "[0.34812957 2.40856513] 2.0 False {}\n",
      "[0.40699558 2.26042347] 2.0 False {}\n",
      "[0.45919049 1.95001773] 2.0000000000000004 False {}\n",
      "[0.50972903 1.92584753] 2.0000000000000013 False {}\n",
      "[ 0.51521113 -1.34113361] -0.5999999999999988 False {}\n",
      "[ 0.43849208 -4.69311227] 0.0 False {}\n",
      "[ 0.28440428 -7.59789813] 0.0 False {}\n",
      "[ 0.06468414 -9.83652142] 0.0 False {}\n",
      "[ -0.19951392 -11.23057929] 0.0 False {}\n",
      "[ -0.49120497 -11.72405068] 8.881784197001252e-16 False {}\n",
      "[ -0.78083193 -11.3923635 ] 1.0942358130705543e-12 False {}\n",
      "[ -1.0538869  -10.44038555] 2.000000000514745 False {}\n",
      "[-1.29906199 -9.10781908] 2.00000006786709 False {}\n",
      "[-1.50716246 -7.61953219] 2.0000026521735492 False {}\n",
      "[-1.62941913 -1.98683672] 2.000018620411578 False {}\n",
      "[-1.61153515  3.24182415] 2.000014134637033 False {}\n",
      "[-1.49675183  5.73206129] 2.000002230950892 False {}\n",
      "[-1.32532632  8.09749888] 2.000000110429627 False {}\n",
      "[-1.09341174 10.22500295] 2.0000000011782926 False {}\n",
      "[-0.81747102 11.90881644] 2.000000000002611 False {}\n",
      "[-0.50610269 12.89261845] 2.000000000000001 False {}\n",
      "[-0.18126773 12.96493049] 2.0 False {}\n",
      "[ 0.135315  12.0412524] 2.0 False {}\n",
      "[ 0.41219238 10.21736965] 2.0 False {}\n",
      "[0.63808374 7.7162884 ] 2.0000000000000324 False {}\n",
      "[0.79737252 4.80745933] 2.000000000001623 False {}\n",
      "[0.88807875 2.54812516] 2.0000000000134244 False {}\n",
      "[0.93932863 1.4664174 ] 2.0000000000426894 False {}\n",
      "[ 0.91756725 -3.12006351] 2.6205704273252195e-11 False {}\n",
      "[ 0.78531214 -7.33673704] 1.2176926134088717e-12 False {}\n",
      "[  0.55480854 -11.00111392] 3.9968028886505635e-15 False {}\n",
      "[  0.24223079 -13.80214041] 0.0 False {}\n",
      "[ -0.12517348 -15.46738676] 0.0 False {}\n",
      "[ -0.51924026 -15.88775356] 1.3322676295501878e-15 False {}\n",
      "[ -0.91115799 -15.22479572] 2.2676971411783597e-11 False {}\n",
      "[ -1.27598025 -13.85233347] 2.0000000439892487 False {}\n",
      "[ -1.60164426 -12.20370588] 2.0000121192732694 False {}\n",
      "[ -1.88567455 -10.62743476] 2.000677093384049 False {}\n",
      "[-2.08458157 -5.18060708] 2.0069638617834618 False {}\n",
      "[-2.1487443  -0.10192215] 2.0135589315591096 False {}\n",
      "[-2.12440677  2.00617411] 2.0105826679810166 False {}\n",
      "[-2.04960298  4.12095055] 2.0047584571944763 False {}\n",
      "[-1.91900703  6.31076257] 2.001028997210702 False {}\n",
      "[-1.73256544  8.59138595] 2.0000856884634652 False {}\n",
      "[-1.48599972 10.89791202] 2.0000018638618084 False {}\n",
      "[-1.18834397 13.04317589] 2.000000008072662 False {}\n",
      "[-0.83891535 14.72505292] 2.0000000000043157 False {}\n",
      "[-0.45881639 15.58344869] 2.0000000000000004 False {}\n",
      "[-0.0701989 15.325535 ] 2.0 False {}\n",
      "[ 0.29714988 13.89008087] 2.0 False {}\n",
      "[ 0.61676796 11.46414648] 2.0000000000000187 False {}\n",
      "[0.86481268 8.38028173] 2.00000000000787 False {}\n",
      "[1.03288138 4.96225661] 2.0000000003293463 False {}\n",
      "[1.12304268 2.27229223] 2.0000000021695765 False {}\n",
      "[ 1.12149034 -2.28208705] 2.000000002101751 False {}\n",
      "[ 1.0057139  -6.90970278] 1.836291119161615e-10 False {}\n",
      "[  0.77979617 -11.08323171] 1.0675904604795505e-12 False {}\n",
      "[  0.45859864 -14.48737024] 4.440892098500626e-16 False {}\n",
      "[  0.06474222 -16.76499913] 0.0 False {}\n",
      "[ -0.3677894 -17.6673419] 0.0 False {}\n",
      "[ -0.80803831 -17.270077  ] 2.0898838215543947e-12 False {}\n",
      "[ -1.22406486 -15.95834553] 2.000000016264014 False {}\n",
      "[ -1.59942102 -14.25388967] 2.000011705780861 False {}\n",
      "[ -1.93693786 -12.63048382] 2.001282834305827 False {}\n",
      "[ -2.23608646 -11.41088426] 2.0314069908584758 False {}\n",
      "[-2.46070623 -6.58681662] 2.1909946491361887 False {}\n",
      "[-2.56980154 -2.31959002] 2.38167877185582 False {}\n",
      "[-2.58156412  1.48528753] 2.4083090693953153 False {}\n",
      "[-2.529008    2.61771014] 2.2988116023684055 False {}\n",
      "[-2.4476119   3.88245866] 2.1743459083656407 False {}\n",
      "[-2.33443955  5.34186323] 2.0737281346010183 False {}\n",
      "[-2.1781643   7.03919771] 2.0181490060147964 False {}\n",
      "[-1.98208421  8.99646654] 2.002202932398869 False {}\n",
      "[-1.72789727 11.16208612] 2.0000801550614633 False {}\n",
      "[-1.4211716  13.38299819] 2.000000615013841 False {}\n",
      "[-1.05966814 15.35721963] 2.0000000005816054 False {}\n",
      "[-0.65759619 16.67173852] 2.0000000000000533 False {}\n",
      "[-0.23619717 16.91463419] 2.0 False {}\n",
      "[ 0.17705407 15.88111897] 2.0 False {}\n",
      "[ 0.54804304 13.66335763] 2.000000000000003 False {}\n",
      "[ 0.85094535 10.60141441] 2.00000000000571 False {}\n",
      "[1.0748313  7.08449522] 2.0000000007998975 False {}\n",
      "[1.22271217 4.64612849] 2.000000015842002 False {}\n",
      "[1.31210816 2.56928411] 2.0000000865086527 False {}\n",
      "[ 1.31654363 -2.20612546] 2.000000093912468 False {}\n",
      "[ 1.1976919  -7.07605807] 2.0000000097088817 False {}\n",
      "[  0.96640818 -11.56678061] 7.782618993701362e-11 False {}\n",
      "[  0.62567901 -15.37501906] 2.353672812205332e-14 False {}\n",
      "[  0.20323755 -18.07658287] 0.0 False {}\n",
      "[ -0.2654277  -19.32117019] 0.0 False {}\n",
      "[ -0.74806278 -19.08903554] 4.969358258222201e-13 False {}\n",
      "[ -1.21074281 -17.77968506] 2.000000012543804 False {}\n",
      "[ -1.63327789 -16.01678476] 2.000019752992935 False {}\n",
      "[ -2.01255029 -14.37981878] 2.0031361976935784 False {}\n",
      "[ -2.35819637 -13.26872801] 2.0892831377154275 False {}\n",
      "[-2.6292448  -8.73274155] 2.5290380015598632 False {}\n",
      "[-2.80084294 -4.94149827] 3.1106275106268817 False {}\n",
      "[-2.8824813 -1.6389649] 3.4233575987797114 False {}\n",
      "[-2.88532862  1.24547283] 3.433978435040254 False {}\n",
      "[-2.85076785  1.49120967] 3.3029939278758125 False {}\n",
      "[-2.81132181  1.84387025] 3.150903616024955 False {}\n",
      "[-2.75992467  2.32494641] 2.956161981857793 False {}\n",
      "[-2.69226959  2.95964417] 2.7191785040601033 False {}\n",
      "[-2.60698549  3.78292943] 2.470124858669653 False {}\n",
      "[-2.50100254  4.84154461] 2.2501388739619546 False {}\n",
      "[-2.36372549  6.17044851] 2.0932743253571595 False {}\n",
      "[-2.19096344  7.80001867] 2.020547216684792 False {}\n",
      "[-1.97270517  9.7286891 ] 2.001972212800285 False {}\n",
      "[-1.70112238 11.88344538] 2.000054424522071 False {}\n",
      "[-1.37708306 14.07597276] 2.0000002823839167 False {}\n",
      "[-1.00007984 15.97220707] 2.0000000001625247 False {}\n",
      "[-0.58528771 17.12101929] 2.0000000000000084 False {}\n",
      "[-0.15383717 17.12664947] 2.0 False {}\n",
      "[ 0.26193752 15.82141469] 2.0 False {}\n",
      "[ 0.6281828  13.36899448] 2.0000000000000253 False {}\n",
      "[ 0.92180308 10.14535304] 2.0000000000288276 False {}\n",
      "[1.1292649  6.53745227] 2.0000000024635267 False {}\n",
      "[1.26047044 3.64222675] 2.0000000327712204 False {}\n",
      "[1.32457673 1.57305013] 2.000000108916471 False {}\n",
      "[ 1.29933214 -3.53148196] 2.0000000682102024 False {}\n",
      "[ 1.14963908 -8.32336838] 2.0000000037243897 False {}\n",
      "[  0.88339668 -12.67409649] 1.206190702873755e-11 False {}\n",
      "[  0.52300631 -16.23499742] 1.7763568394002505e-15 False {}\n",
      "[  0.08378701 -18.57713667] 0.0 False {}\n",
      "[ -0.39230271 -19.41506489] 0.0 False {}\n",
      "[ -0.87431241 -18.84414705] 9.794387523243131e-12 False {}\n",
      "[ -1.32763014 -17.35197655] 2.0000001152090103 False {}\n",
      "[ -1.73949096 -15.5720216 ] 2.0000945706229314 False {}\n",
      "[ -2.10685092 -14.0414194 ] 2.0088173484721 False {}\n",
      "[ -2.44746965 -13.12062337] 2.174171597737662 False {}\n",
      "[-2.72054324 -8.82248845] 2.8146660304990014 False {}\n",
      "[-2.89422685 -5.29536045] 3.4669065783693864 False {}\n",
      "[-2.98881573 -2.28858073] 3.776955249996174 False {}\n",
      "[-3.01241256  0.40039277] 3.837870076267671 False {}\n",
      "[-3.00308405  0.24519957] 3.8147662931231583 False {}\n",
      "[-2.9994331   0.11716265] 3.8053698450577897 False {}\n",
      "[-2.9990281   0.00305944] 3.804315491037074 False {}\n",
      "[-2.99694456e+00  9.47602948e-04] 3.7988537682794403 False {}\n",
      "[-2.99724709e+00 -1.37297782e-03] 3.799650691564734 False {}\n",
      "[-2.99584246e+00 -7.15217227e-04] 3.7959394959234998 False {}\n",
      "[-2.99912324e+00  2.22323133e-06] 3.8045633981883116 False {}\n",
      "[-2.99728688e+00  1.21360686e-03] 3.7997554208174984 False {}\n",
      "[-2.99838749e+00 -1.31468486e-03] 3.8026429031355464 False {}\n",
      "[-2.99687556e+00 -6.26340078e-04] 3.798671812520978 False {}\n",
      "[-2.99456088e+00 -6.86347757e-04] 3.7925288212651918 False {}\n",
      "[-2.99656372e+00  1.26631947e-03] 3.7978486821255384 False {}\n",
      "[-2.99660123e+00  1.28895062e-03] 3.797947778651362 False {}\n",
      "[-2.99677312e+00  1.70475612e-04] 3.798401572538319 False {}\n",
      "[-2.99735243e+00  1.13225358e-03] 3.799927883103478 False {}\n",
      "[-2.99872831e+00 -1.04345709e-03] 3.803533498936436 False {}\n",
      "[-2.99754338e+00 -5.21102053e-04] 3.8004299039418994 False {}\n",
      "[-2.99615923e+00  3.99587621e-04] 3.7967789112464945 False {}\n",
      "[-2.99672921e+00 -2.11390917e-04] 3.7982856770106452 False {}\n",
      "[-2.99796189e+00  2.42291590e-04] 3.801528381501792 False {}\n",
      "[-2.99786378e+00  1.27363545e-03] 3.801271093773407 False {}\n",
      "[-2.99848444e+00 -4.27509261e-04] 3.8028964073734857 False {}\n",
      "[-2.99829093e+00  7.93991650e-04] 3.802390277395504 False {}\n",
      "[-2.99807532e+00 -2.40798877e-04] 3.801825679884376 False {}\n",
      "[-2.99665522e+00  8.25334046e-04] 3.798090343690541 False {}\n",
      "[-2.99724541e+00  5.49613233e-04] 3.7996462660882244 False {}\n",
      "[-2.99695489e+00  1.08880006e-03] 3.7988810035093454 False {}\n",
      "[-2.99804205e+00  1.32187979e-03] 3.801738493277284 False {}\n",
      "[-2.99664251e+00 -8.42412237e-04] 3.798056794166685 False {}\n",
      "[-2.99845143e+00  5.23589103e-05] 3.80281010377484 False {}\n",
      "[-2.99628259e+00  5.75862873e-04] 3.797105415946533 False {}\n",
      "[-2.99640644e+00  1.99901222e-04] 3.7974329865120624 False {}\n",
      "[-2.99824286e+00  7.28582772e-04] 3.8022644611034226 False {}\n",
      "[-2.99700897e+00 -8.77528376e-04] 3.799023546323099 False {}\n",
      "[-2.99737798e+00  1.86812923e-03] 3.799995070740911 False {}\n",
      "[-2.99507617e+00 -7.02019173e-04] 3.793902989158593 False {}\n",
      "[-2.99814903e+00 -2.13310869e-03] 3.8020187810016717 False {}\n",
      "[-2.99696107e+00 -2.84291704e-04] 3.798897281957341 False {}\n",
      "[-2.99752047e+00 -2.94571269e-04] 3.800369713144444 False {}\n",
      "[-2.99621520e+00  1.12319216e-03] 3.7969270755280347 False {}\n",
      "[-2.99635241e+00 -2.06081431e-04] 3.7972901197601594 False {}\n",
      "[-2.99819525e+00  9.22043926e-04] 3.8021398087611535 False {}\n",
      "[-2.99680027e+00 -2.54986593e-04] 3.7984732039212714 False {}\n",
      "[-2.99942036e+00  1.33939121e-03] 3.805336719825311 False {}\n",
      "[-2.99758979e+00  1.45036032e-03] 3.800551852831922 False {}\n",
      "[-2.99619535e+00  6.32438368e-04] 3.796874541476468 False {}\n",
      "[-2.99458367e+00  6.13750612e-05] 3.792589681765322 False {}\n",
      "[-2.99771020e+00  1.16869469e-03] 3.800868083605729 False {}\n",
      "[-2.99575381e+00 -4.19882692e-04] 3.7957043325943562 False {}\n",
      "[-2.99592445e+00 -4.12501942e-04] 3.79615688978373 False {}\n",
      "[-2.99690859e+00 -1.04017802e-03] 3.79875891761252 False {}\n",
      "[-2.99892011e+00  9.57488988e-04] 3.8040339603897637 False {}\n",
      "[-2.99715760e+00  2.13909992e-04] 3.7994150921643075 False {}\n",
      "[-2.99796789e+00 -5.14034037e-04] 3.801544125683587 False {}\n",
      "[-2.99798326e+00  7.33632044e-04] 3.801584416384541 False {}\n",
      "[-2.99680364e+00 -6.33676073e-04] 3.7984820923799703 False {}\n",
      "[-2.99639948e+00 -5.81479306e-04] 3.7974145907555314 False {}\n",
      "[-2.99594544e+00 -2.23931635e-04] 3.796212539460031 False {}\n",
      "[-2.99686991e+00  6.16894480e-04] 3.7986569140365534 False {}\n",
      "[-2.99813697e+00 -4.03215035e-04] 3.80198717796229 False {}\n",
      "[-2.99540618e+00 -6.50380206e-04] 3.7947810594961036 False {}\n",
      "[-2.99786581e+00  1.07355445e-03] 3.8012764316656913 False {}\n",
      "[-2.99687531e+00 -1.35034716e-04] 3.798671159608567 False {}\n",
      "[-2.99562992e+00  9.68614411e-04] 3.795375471880499 False {}\n",
      "[-2.99807660e+00  7.05524121e-04] 3.801829030799799 False {}\n",
      "[-2.99694513e+00  6.08081718e-04] 3.798855272042216 False {}\n",
      "[-2.99611737e+00  3.24162875e-04] 3.7966680719366455 False {}\n",
      "[-2.99914058e+00  1.95724427e-03] 3.804608544846356 False {}\n",
      "[-2.99717880e+00 -1.51126454e-03] 3.7994709094760424 False {}\n",
      "[-2.99705767e+00  1.53261122e-03] 3.799151862848566 False {}\n",
      "[-2.99734916e+00 -8.78442541e-04] 3.799919266762103 False {}\n",
      "[-2.99642954e+00  2.20064707e-03] 3.7974940753013504 False {}\n",
      "[-2.99533189e+00  1.35646660e-03] 3.794583520294644 False {}\n",
      "[-2.99661564e+00  3.98520584e-04] 3.797985828295471 False {}\n",
      "[-2.99743273e+00  2.48822479e-04] 3.8001390539960687 False {}\n",
      "[-2.99594178e+00  1.83560783e-03] 3.7962028432001356 False {}\n",
      "[-2.99738008e+00  3.55603858e-04] 3.8000006119942347 False {}\n",
      "[-2.99663789e+00  1.04443974e-03] 3.798044591439679 False {}\n",
      "[-2.99562912e+00 -7.86644584e-04] 3.795373366238411 False {}\n",
      "[-2.99822096e+00 -2.22131874e-03] 3.802207116125385 False {}\n",
      "[-2.99825167e+00  6.73982936e-04] 3.8022875133546807 False {}\n",
      "[-2.99642998e+00 -2.51467748e-04] 3.7974952392178967 False {}\n",
      "[-2.99695240e+00  1.36069082e-03] 3.7988744394705662 False {}\n",
      "[-2.99625429e+00 -5.71719725e-04] 3.7970305190960776 False {}\n",
      "[-2.99547135e+00 -2.97443006e-04] 3.7949542651652655 False {}\n",
      "[-2.99688872e+00  1.76205686e-03] 3.7987065378130054 False {}\n",
      "[-2.9976185e+00  1.4438802e-03] 3.800627272987933 False {}\n",
      "[-2.99589046e+00  4.87869139e-04] 3.796066785843528 False {}\n",
      "[-2.99715520e+00  1.03395627e-03] 3.7994087780414336 False {}\n",
      "[-2.99725522e+00 -4.88182770e-04] 3.799672086239772 False {}\n",
      "[-2.99744916e+00  5.83221865e-04] 3.800182258578559 False {}\n",
      "[-2.99542170e+00 -7.89574042e-04] 3.7948223183885146 False {}\n",
      "[-2.99566467e+00 -2.82240289e-03] 3.7954677453573495 False {}\n",
      "[-2.99631833e+00  1.53744520e-04] 3.797199979119286 False {}\n",
      "[-2.99685192e+00  9.07734705e-04] 3.798609480538053 False {}\n",
      "[-2.99798470e+00  4.13821014e-04] 3.80158818304756 False {}\n",
      "[-2.99675440e+00  4.95823983e-05] 3.7983521764184185 False {}\n",
      "[-2.99692796e+00  9.71481750e-04] 3.798809996628674 False {}\n",
      "[-2.99725189e+00  5.27221909e-04] 3.7996633233069517 False {}\n",
      "[-2.99549837e+00  3.07899219e-04] 3.7950260693926303 False {}\n",
      "[-2.99795188e+00  1.12671700e-03] 3.8015021564639864 False {}\n",
      "[-2.99615862e+00  1.25795425e-03] 3.7967772991611133 False {}\n",
      "[-2.99892867e+00 -9.13746753e-04] 3.8040562775831344 False {}\n",
      "[-2.99703482e+00  9.00914034e-04] 3.799091669280561 False {}\n",
      "[-2.99616586e+00  1.32078416e-04] 3.796796459253257 False {}\n",
      "[-2.99765082e+00 -1.36874796e-03] 3.800712144627804 False {}\n",
      "[-2.99641075e+00 -3.98388319e-04] 3.7974443786780228 False {}\n",
      "[-2.99517624e+00  1.32266714e-03] 3.7941694106580983 False {}\n",
      "[-2.99537129e+00 -1.78134902e-03] 3.794688286025653 False {}\n",
      "[-2.99751252e+00  1.97209072e-03] 3.8003488060247332 False {}\n",
      "[-2.99637583e+00 -6.88847328e-04] 3.797352040138668 False {}\n",
      "[-2.99770258e+00 -3.13865950e-04] 3.800848077494097 False {}\n",
      "[-2.99557469e+00  5.81374515e-04] 3.7952288037719244 False {}\n",
      "[-2.99704076e+00  1.36652604e-03] 3.799107315514056 False {}\n",
      "[-2.99783309e+00  8.33794916e-04] 3.801190599535457 False {}\n",
      "[-2.99678980e+00  5.63213393e-04] 3.7984455883406207 False {}\n",
      "[-2.99614400e+00  2.07259777e-03] 3.7967385889767007 False {}\n",
      "[-2.99545431e+00 -1.95198832e-05] 3.7949089752023646 False {}\n",
      "[-2.99529335e+00  6.41428338e-04] 3.794481013324229 False {}\n",
      "[-2.99641400e+00  2.44259499e-04] 3.7974529923602187 False {}\n",
      "[-2.99520726e+00  1.12249486e-03] 3.7942519623033193 False {}\n",
      "[-2.99588304e+00  1.16272894e-03] 3.796047099536615 False {}\n",
      "[-2.99736538e+00 -2.47533300e-04] 3.79996194840659 False {}\n",
      "[-2.99464582e+00 -1.74003529e-03] 3.792755600989742 False {}\n",
      "[-2.99652244e+00 -5.80167723e-04] 3.7977396061140167 False {}\n",
      "[-2.99671001e+00  2.53332267e-04] 3.798235002940566 False {}\n",
      "[-2.99730639e+00  8.20237369e-05] 3.7998067408131844 False {}\n",
      "[-2.99815119e+00 -9.90119104e-04] 3.8020244222409207 False {}\n",
      "[-2.99527815e+00 -6.82946127e-04] 3.7944405876028875 False {}\n",
      "[-2.99541578e+00  7.96793925e-04] 3.794806570948769 False {}\n",
      "[-2.99762928e+00 -5.00331768e-04] 3.8006555697295195 False {}\n",
      "[-2.99878513e+00  2.18237932e-03] 3.803681801092842 False {}\n",
      "[-2.99825993e+00  1.50752503e-03] 3.8023091316735167 False {}\n",
      "[-2.99545598e+00 -6.26094284e-04] 3.794913434256373 False {}\n",
      "[-2.99702871e+00  8.85726920e-04] 3.7990755744461007 False {}\n",
      "[-2.99488859e+00  9.30158520e-04] 3.7934031773055334 False {}\n",
      "[-2.99760817e+00  8.06258081e-04] 3.800600136129094 False {}\n",
      "[-2.99789933e+00 -1.64222460e-03] 3.801364350480359 False {}\n",
      "[-2.99472003e+00 -1.86472757e-03] 3.7929536366514407 False {}\n",
      "[-2.99475571e+00  1.55969831e-03] 3.793048835858616 False {}\n",
      "[-2.99654798e+00  3.15848939e-04] 3.7978071112314415 False {}\n",
      "[-2.99548840e+00  2.13881664e-04] 3.7949995821590568 False {}\n",
      "[-2.99636407e+00 -2.51169476e-04] 3.7973209510656423 False {}\n",
      "[-2.99681348e+00  6.08663702e-04] 3.798508075380579 False {}\n",
      "[-2.99522847e+00  1.37484453e-03] 3.7943084032123853 False {}\n",
      "[-2.99611261e+00 -6.77064195e-04] 3.7966554490301516 False {}\n",
      "[-2.99540532e+00  9.55866633e-04] 3.794778767921583 False {}\n",
      "[-2.99605479e+00  4.34016635e-04] 3.7965023039129804 False {}\n",
      "[-2.99563377e+00 -1.08505026e-03] 3.7953857075907336 False {}\n",
      "[-2.99614513e+00 -1.91571556e-03] 3.7967415719444313 False {}\n",
      "[-2.99733076e+00  3.42537481e-04] 3.7998708595591824 False {}\n",
      "[-2.99681099e+00  5.11134326e-04] 3.7985015054769518 False {}\n",
      "[-2.99533490e+00  4.77807722e-05] 3.7945915252641442 False {}\n",
      "[-2.99467717e+00  7.43951563e-04] 3.7928392665494206 False {}\n",
      "[-2.99468924e+00  3.31883785e-04] 3.792871493752436 False {}\n",
      "[-2.99676890e+00 -1.66952951e-03] 3.7983904243575246 False {}\n",
      "[-2.99591928e+00 -2.24513888e-04] 3.7961431994526516 False {}\n",
      "[-2.99677136e+00  3.37389595e-04] 3.7983969376349345 False {}\n",
      "[-2.9947364e+00 -9.1911272e-04] 3.792997325972385 False {}\n",
      "[-2.99618915e+00  4.56038218e-04] 3.7968581132969956 False {}\n",
      "[-2.99962356e+00 -1.94157081e-04] 3.8058648551834664 True {'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_episode_steps = 1000 \n",
    "\n",
    "env = gym.make('unbalanced-disk-v0', dt=0.025, umax=3.)\n",
    "env = gym.wrappers.time_limit.TimeLimit(env,max_episode_steps=max_episode_steps) #c)\n",
    "env = Discretize(env, num_act=11)\n",
    "\n",
    "# Define training (Hyper)-parameters\n",
    "gamma = 0.99\n",
    "batch_size = 32 \n",
    "N_iterations = 10\n",
    "N_rollout = 2000#0\n",
    "N_epochs = 5\n",
    "N_evals = 10\n",
    "alpha_actor = 0.5\n",
    "alpha_entropy = 0.6\n",
    "lr = 5e-3\n",
    "\n",
    "\n",
    "assert isinstance(env.action_space,gym.spaces.Discrete), 'action space requires to be discrete'\n",
    "\n",
    "actor_crit = ActorCritic(env, num_hidden_act=40,num_hidden_cri=40)\n",
    "optimizer = torch.optim.Adam(actor_crit.parameters(), lr=lr) #low learning rate\n",
    "\n",
    "A2C_train(actor_crit, optimizer, env, N_iter=N_iterations,N_rollout=N_rollout,N_epochs=N_epochs, batch_size=batch_size ,N_evals=N_evals,\\\n",
    "              alpha_actor=alpha_actor,alpha_entropy=alpha_entropy,gamma=gamma)\n",
    "\n",
    "plt.plot([eval_actor(actor_crit, env) for i in range(100)],'.')\n",
    "plt.show()\n",
    "show(actor_crit,env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02286364 1.90355445] 2.0 False {}\n",
      "[0.08227019 2.77543255] 2.0 False {}\n",
      "[0.15037921 2.58431806] 2.0 False {}\n",
      "[0.21868914 3.01627959] 2.0 False {}\n",
      "[0.28673583 2.38731575] 2.0 False {}\n",
      "[0.34874661 2.4155977 ] 2.0 False {}\n",
      "[0.40689953 2.26583864] 2.0 False {}\n",
      "[0.46187888 1.95553165] 2.0000000000000004 False {}\n",
      "[0.50910272 1.93078932] 2.0000000000000013 False {}\n",
      "[ 0.51595258 -1.33767344] -0.5999999999999988 False {}\n",
      "[ 0.44026213 -4.69151795] 0.0 False {}\n",
      "[ 0.28397341 -7.59736652] 0.0 False {}\n",
      "[ 0.06299977 -9.83741947] 0.0 False {}\n",
      "[ -0.20059255 -11.23533493] 0.0 False {}\n",
      "[ -0.49041686 -11.72897455] 8.881784197001252e-16 False {}\n",
      "[ -0.77934796 -11.39888291] 1.056044141023449e-12 False {}\n",
      "[ -1.05493287 -10.44396841] 2.000000000526258 False {}\n",
      "[-1.29839036 -9.11287452] 2.0000000670212974 False {}\n",
      "[-1.50743141 -7.62525714] 2.0000026640110833 False {}\n",
      "[-1.62736156 -1.99171634] 2.0000180421719884 False {}\n",
      "[-1.61272226  3.23728395] 2.0000143970688247 False {}\n",
      "[-1.5010018   5.72709746] 2.0000023944777356 False {}\n",
      "[-1.32567966  8.09665524] 2.000000111149947 False {}\n",
      "[-1.09760398 10.22222223] 2.00000000128528 False {}\n",
      "[-0.81861378 11.90572367] 2.000000000002682 False {}\n",
      "[-0.50855357 12.89216563] 2.0000000000000013 False {}\n",
      "[-0.18246325 12.96729528] 2.0 False {}\n",
      "[ 0.13257419 12.04593849] 2.0 False {}\n",
      "[ 0.41423388 10.22504521] 2.0 False {}\n",
      "[0.63737132 7.72423911] 2.000000000000032 False {}\n",
      "[0.79535548 4.81478231] 2.000000000001547 False {}\n",
      "[0.8878622  2.55493348] 2.000000000013358 False {}\n",
      "[0.93836476 1.47647452] 2.000000000041781 False {}\n",
      "[ 0.91692822 -3.11406125] 2.5830892980138742e-11 False {}\n",
      "[ 0.78527866 -7.33387007] 1.2168044349891716e-12 False {}\n",
      "[  0.5551244  -10.99831679] 3.9968028886505635e-15 False {}\n",
      "[  0.24252119 -13.80094268] 0.0 False {}\n",
      "[ -0.12834805 -15.46660196] 0.0 False {}\n",
      "[ -0.52130608 -15.88995969] 1.7763568394002505e-15 False {}\n",
      "[ -0.91200494 -15.23037897] 2.311484337269576e-11 False {}\n",
      "[ -1.2750438  -13.85828801] 2.0000000432172382 False {}\n",
      "[ -1.60105991 -12.21002208] 2.000012009256705 False {}\n",
      "[ -1.8854318  -10.63218077] 2.0006750048274524 False {}\n",
      "[-2.08339995 -5.18715634] 2.0068762416968275 False {}\n",
      "[-2.14812977 -0.10635118] 2.0134753438152218 False {}\n",
      "[-2.1270511   2.00549343] 2.0108746594674773 False {}\n",
      "[-2.04770514  4.11584891] 2.004659495992914 False {}\n",
      "[-1.91911769  6.30638033] 2.0010304086421846 False {}\n",
      "[-1.73287355  8.58855949] 2.0000860661684374 False {}\n",
      "[-1.48901171 10.89408397] 2.0000019603629395 False {}\n",
      "[-1.18890505 13.04168541] 2.0000000081627864 False {}\n",
      "[-0.84004243 14.72567653] 2.0000000000044307 False {}\n",
      "[-0.45971072 15.58264795] 2.0000000000000004 False {}\n",
      "[-0.07270589 15.32715764] 2.0 False {}\n",
      "[ 0.29566542 13.89217118] 2.0 False {}\n",
      "[ 0.61621851 11.47092745] 2.0000000000000187 False {}\n",
      "[0.86538126 8.38595735] 2.000000000007974 False {}\n",
      "[1.03084064 4.96817861] 2.000000000315288 False {}\n",
      "[1.12162112 2.27924897] 2.000000002107385 False {}\n",
      "[ 1.12312833 -2.27759375] 2.00000000217338 False {}\n",
      "[ 1.00606584 -6.905883  ] 1.8503287790849754e-10 False {}\n",
      "[  0.77827213 -11.07841718] 1.0293987884324451e-12 False {}\n",
      "[  0.45882568 -14.48467617] 4.440892098500626e-16 False {}\n",
      "[  0.06409226 -16.76195442] 0.0 False {}\n",
      "[ -0.3675816  -17.66746923] 0.0 False {}\n",
      "[ -0.80636879 -17.27136567] 2.0090595853616833e-12 False {}\n",
      "[ -1.2244199  -15.95891294] 2.000000016376581 False {}\n",
      "[ -1.60204746 -14.25471855] 2.000012195747072 False {}\n",
      "[ -1.93770789 -12.63274017] 2.001294944311666 False {}\n",
      "[ -2.23625706 -11.40841844] 2.031456180414832 False {}\n",
      "[-2.46215046 -6.58802695] 2.192905093016693 False {}\n",
      "[-2.57182544 -2.31908611] 2.386172429044292 False {}\n",
      "[-2.58231832  1.48737524] 2.41005900555749 False {}\n",
      "[-2.52919311  2.6199706 ] 2.299155070747102 False {}\n",
      "[-2.44705758  3.88407175] 2.173667408155705 False {}\n",
      "[-2.33281497  5.34420164] 2.0727540845645067 False {}\n",
      "[-2.17932651  7.0458369 ] 2.018355952539785 False {}\n",
      "[-1.97890738  9.00365575] 2.002122120804977 False {}\n",
      "[-1.72762322 11.16919289] 2.0000798410065457 False {}\n",
      "[-1.41996902 13.38838246] 2.0000006022513035 False {}\n",
      "[-1.05986523 15.36269229] 2.0000000005840284 False {}\n",
      "[-0.65748722 16.67561266] 2.000000000000053 False {}\n",
      "[-0.23479272 16.91447859] 2.0 False {}\n",
      "[ 0.17664741 15.87313014] 2.0 False {}\n",
      "[ 0.54952793 13.65491932] 2.000000000000003 False {}\n",
      "[ 0.85349892 10.59514688] 2.0000000000060583 False {}\n",
      "[1.07532463 7.07475939] 2.000000000808203 False {}\n",
      "[1.22198722 4.63343071] 2.00000001562024 False {}\n",
      "[1.31057688 2.56017945] 2.0000000840866536 False {}\n",
      "[ 1.31608258 -2.21479301] 2.000000093115106 False {}\n",
      "[ 1.19979078 -7.08290063] 2.000000010118425 False {}\n",
      "[  0.96282827 -11.57290328] 7.191713891074869e-11 False {}\n",
      "[  0.62566695 -15.38157575] 2.353672812205332e-14 False {}\n",
      "[  0.20508721 -18.08059598] 0.0 False {}\n",
      "[ -0.26670762 -19.32048653] 0.0 False {}\n",
      "[ -0.75287029 -19.08390562] 5.582201367815287e-13 False {}\n",
      "[ -1.21209457 -17.77560161] 2.000000012879835 False {}\n",
      "[ -1.63487901 -16.01081621] 2.000020242028987 False {}\n",
      "[ -2.01555437 -14.37324689] 2.0032456994863064 False {}\n",
      "[ -2.357052   -13.26525461] 2.088475224707178 False {}\n",
      "[-2.63180567 -8.73260167] 2.5361001355215333 False {}\n",
      "[-2.79928246 -4.94190705] 3.104646388192221 False {}\n",
      "[-2.88340257 -1.64113426] 3.426798218817344 False {}\n",
      "[-2.88581115  1.2438809 ] 3.435774492139813 False {}\n",
      "[-2.85235867  1.48899054] 3.3090994020595925 False {}\n",
      "[-2.81220057  1.84026119] 3.1542884432036598 False {}\n",
      "[-2.75789206  2.32031622] 2.9486558453201566 False {}\n",
      "[-2.69456092  2.95305939] 2.7267005520311223 False {}\n",
      "[-2.60861801  3.77347105] 2.474294140096829 False {}\n",
      "[-2.50184803  4.82761567] 2.251514414132285 False {}\n",
      "[-2.36400157  6.15307818] 2.0934774682199544 False {}\n",
      "[-2.19266978  7.77988895] 2.0208873986555504 False {}\n",
      "[-1.97286344  9.70578462] 2.0019759129615293 False {}\n",
      "[-1.70576726 11.86040172] 2.000058235650967 False {}\n",
      "[-1.38154797 14.05373127] 2.0000003058183524 False {}\n",
      "[-1.00577063 15.95354354] 2.0000000001838547 False {}\n",
      "[-0.58928737 17.11569206] 2.0000000000000093 False {}\n",
      "[-0.15867611 17.13425523] 2.0 False {}\n",
      "[ 0.25681815 15.84275469] 2.0 False {}\n",
      "[ 0.62512873 13.4022973 ] 2.0000000000000235 False {}\n",
      "[ 0.9201576  10.18477657] 2.00000000002778 False {}\n",
      "[1.12923722 6.57813955] 2.0000000024621363 False {}\n",
      "[1.25820161 3.68274694] 2.000000031383468 False {}\n",
      "[1.32531512 1.61482546] 2.0000001104068734 False {}\n",
      "[ 1.30040441 -3.49531522] 2.000000069588779 False {}\n",
      "[ 1.15170288 -8.28984069] 2.0000000038827146 False {}\n",
      "[  0.88872963 -12.6455289 ] 1.3625545136619621e-11 False {}\n",
      "[  0.52598877 -16.21655012] 1.7763568394002505e-15 False {}\n",
      "[  0.09056531 -18.57036932] 0.0 False {}\n",
      "[ -0.38944653 -19.41978538] 0.0 False {}\n",
      "[ -0.87205926 -18.85743224] 9.299672143470161e-12 False {}\n",
      "[ -1.32434953 -17.36770959] 2.00000010846181 False {}\n",
      "[ -1.73887067 -15.5877488 ] 2.000093740742631 False {}\n",
      "[ -2.10734426 -14.05561281] 2.0088630619483347 False {}\n",
      "[ -2.44460804 -13.12987035] 2.1706942505047437 False {}\n",
      "[-2.71959666 -8.82559722] 2.8113791946761575 False {}\n",
      "[-2.89405051 -5.29356711] 3.466258179717449 False {}\n",
      "[-2.98486549 -2.28182695] 3.7659830947072406 False {}\n",
      "[-3.00958075  0.41386887] 3.830996294501091 False {}\n",
      "[-2.99994195  0.26771805] 3.806691189867358 False {}\n",
      "[-2.99567485  0.14973887] 3.795494765753072 False {}\n",
      "[-2.99211493  0.04449234] 3.7859548983321085 False {}\n",
      "[-2.99166369e+00  2.00343662e-03] 3.7847329332577146 False {}\n",
      "[-2.99221639e+00 -4.66274568e-04] 3.786229272606167 False {}\n",
      "[-2.99356986e+00  1.26257554e-03] 3.789875460471918 False {}\n",
      "[-2.99294596e+00  1.31756886e-03] 3.788197892585826 False {}\n",
      "[-2.99414966e+00  7.00835349e-04] 3.7914295166723506 False {}\n",
      "[-2.99483506e+00  2.94236642e-04] 3.79326047768152 False {}\n",
      "[-2.99296735e+00  8.76106783e-04] 3.7882555055352722 False {}\n",
      "[-2.99105228e+00 -3.68287362e-05] 3.7830726718758765 False {}\n",
      "[-2.99235026e+00  7.66174188e-04] 3.7865910526796007 False {}\n",
      "[-2.99280033e+00  1.17484129e-03] 3.7878055424869617 False {}\n",
      "[-2.99286548e+00  7.29987266e-04] 3.787981111807936 False {}\n",
      "[-2.99281371e+00  2.87459048e-03] 3.787841594668082 False {}\n",
      "[-2.99332434e+00  2.27936220e-03] 3.7892159537544807 False {}\n",
      "[-2.99180315e+00  3.16293356e-04] 3.785110891658211 False {}\n",
      "[-2.99338922e+00  7.62545965e-04] 3.789390295653964 False {}\n",
      "[-2.99334828e+00 -1.06636184e-03] 3.7892802792261873 False {}\n",
      "[-2.99403624e+00  6.52260940e-04] 3.7911258912496986 False {}\n",
      "[-2.99292012e+00  4.28183207e-04] 3.7881282981144264 False {}\n",
      "[-2.99365464e+00 -5.46182508e-04] 3.7901029979267653 False {}\n",
      "[-2.99217076e+00 -2.63769345e-04] 3.786105904458611 False {}\n",
      "[-2.99189996e+00  9.93194334e-04] 3.7853731116353506 False {}\n",
      "[-2.99219200e+00  9.06588952e-04] 3.7861633347942343 False {}\n",
      "[-2.99116968e+00  5.42654561e-04] 3.783391871067873 False {}\n",
      "[-2.99338230e+00 -3.14209272e-04] 3.7893717111099035 False {}\n",
      "[-2.99219761e+00  1.41627535e-04] 3.786178504630686 False {}\n",
      "[-2.99070156e+00 -1.08962555e-03] 3.7821179652869756 False {}\n",
      "[-2.99312345e+00  1.12612137e-03] 3.788675685508495 False {}\n",
      "[-2.99130319e+00 -8.56807365e-04] 3.7837546505575173 False {}\n",
      "[-2.99331303e+00 -1.63742830e-04] 3.7891855454311116 False {}\n",
      "[-2.99242448e+00  7.79791873e-04] 3.786791518488476 False {}\n",
      "[-2.99341549e+00  3.95259555e-04] 3.7894608875908773 False {}\n",
      "[-2.99204743e+00  9.39587952e-04] 3.7857723027796366 False {}\n",
      "[-2.99237328e+00 -1.13425396e-04] 3.786653243494981 False {}\n",
      "[-2.99253803e+00  6.54109410e-04] 3.787098073705893 False {}\n",
      "[-2.99218843e+00 -3.65247425e-05] 3.7861536759638583 False {}\n",
      "[-2.99268598e+00  8.65887403e-04] 3.7874972490928247 False {}\n",
      "[-2.99109514e+00 -3.17772009e-04] 3.783189219606302 False {}\n",
      "[-2.99212679e+00 -8.42340174e-04] 3.7859869813032185 False {}\n",
      "[-2.99253649e+00 -2.48874999e-04] 3.7870939316875782 False {}\n",
      "[-2.99146137e+00  3.28138924e-04] 3.7841841067830186 False {}\n",
      "[-2.99382619e+00 -3.05764188e-06] 3.790563077146756 False {}\n",
      "[-2.99324695e+00  1.35625411e-03] 3.7890078862362953 False {}\n",
      "[-2.99257607e+00 -2.33890222e-03] 3.787200730875185 False {}\n",
      "[-2.99281271e+00 -3.84514531e-04] 3.7878388965087613 False {}\n",
      "[-2.99127150e+00 -1.28544911e-03] 3.783668555010106 False {}\n",
      "[-2.99349885e+00  1.98209642e-03] 3.7896847973237424 False {}\n",
      "[-2.99182202e+00 -9.92767891e-04] 3.78516203129292 False {}\n",
      "[-2.99315768e+00  1.82316072e-03] 3.7887677735710357 False {}\n",
      "[-2.99052015e+00  1.42673536e-03] 3.781623481585898 False {}\n",
      "[-2.99224763e+00 -2.68666380e-04] 3.7863137099958886 False {}\n",
      "[-2.99158657e+00  2.19611554e-04] 3.7845238050905596 False {}\n",
      "[-2.99248781e+00 -4.99875532e-04] 3.786962521833345 False {}\n",
      "[-2.99197568e+00  9.98744604e-04] 3.785578110942232 False {}\n",
      "[-2.99251952e+00  1.48646552e-03] 3.787048112138199 False {}\n",
      "[-2.99122819e+00 -6.53112260e-04] 3.7835508925502173 False {}\n",
      "[-2.99296573e+00 -5.99373303e-04] 3.7882511313319798 False {}\n",
      "[-2.99095457e+00 -5.32528999e-04] 3.782806875935047 False {}\n",
      "[-2.99125589e+00  6.93218251e-04] 3.783626143338978 False {}\n",
      "[-2.99050369e+00  2.29193249e-04] 3.781578582910339 False {}\n",
      "[-2.99089965e+00  1.51098465e-03] 3.7826574086840106 False {}\n",
      "[-2.99288068e+00 -6.47020690e-04] 3.7880220503622457 False {}\n",
      "[-2.99324695e+00 -7.24539390e-05] 3.789007899304969 False {}\n",
      "[-2.99163043e+00  1.47857676e-03] 3.7846427617544904 False {}\n",
      "[-2.99152409e+00  1.66535720e-03] 3.7843543170977156 False {}\n",
      "[-2.99152979e+00  1.60596707e-03] 3.7843697852517035 False {}\n",
      "[-2.99199997e+00  6.27191697e-04] 3.7856438689829597 False {}\n",
      "[-2.99137892e+00  2.06576863e-04] 3.783960297860906 False {}\n",
      "[-2.99214453e+00 -2.60817657e-04] 3.78603494854699 False {}\n",
      "[-2.98928023e+00  8.70897474e-04] 3.7782314346107784 False {}\n",
      "[-2.99084142e+00 -1.56193423e-04] 3.7824988792150522 False {}\n",
      "[-2.99066733e+00  4.84944931e-04] 3.7820246964928463 False {}\n",
      "[-2.99337434e+00  1.82156680e-03] 3.789350320256885 False {}\n",
      "[-2.99236890e+00 -7.84902468e-04] 3.7866413990220846 False {}\n",
      "[-2.99222284e+00  9.09789965e-04] 3.7862467166662572 False {}\n",
      "[-2.99156099e+00  1.41850565e-04] 3.784454416231652 False {}\n",
      "[-2.99198667e+00  1.18837496e-03] 3.785607874107976 False {}\n",
      "[-2.99371304e+00  8.15770806e-04] 3.7902596551169387 False {}\n",
      "[-2.99196076e+00  7.56135706e-04] 3.7855377309854132 False {}\n",
      "[-2.99258940e+00 -2.78783587e-04] 3.7872367095057626 False {}\n",
      "[-2.99129551e+00 -2.86918565e-04] 3.7837337869351573 False {}\n",
      "[-2.99042050e+00 -6.88171136e-06] 3.781351655029292 False {}\n",
      "[-2.99123750e+00 -1.23917102e-03] 3.783576184189099 False {}\n",
      "[-2.99309602e+00 -6.29552176e-04] 3.7886018773825496 False {}\n",
      "[-2.99151041e+00 -8.39799458e-04] 3.7843172008997414 False {}\n",
      "[-2.99058191e+00 -6.60818561e-04] 3.781791877867721 False {}\n",
      "[-2.99216580e+00  4.28829413e-04] 3.7860924952158905 False {}\n",
      "[-2.99126167e+00  9.15224520e-04] 3.7836418591776093 False {}\n",
      "[-2.99131863e+00 -2.91151294e-06] 3.7837965679975136 False {}\n",
      "[-2.99210048e+00  1.38364636e-03] 3.7859158165354163 False {}\n",
      "[-2.99160844e+00  6.34490591e-04] 3.784583129040188 False {}\n",
      "[-2.99109784e+00  1.32112067e-03] 3.783196578303098 False {}\n",
      "[-2.99123860e+00  1.48485349e-03] 3.783579170679668 False {}\n",
      "[-2.99079691e+00 -1.72815233e-04] 3.782377695304775 False {}\n",
      "[-2.99147162e+00  2.48870695e-04] 3.784211927502457 False {}\n",
      "[-2.99205509e+00  7.65415819e-04] 3.7857930254416923 False {}\n",
      "[-2.99084036e+00 -5.27570698e-04] 3.7824960061348474 False {}\n",
      "[-2.99194858e+00  1.23326938e-03] 3.7855047503629025 False {}\n",
      "[-2.99180977e+00 -1.39079379e-04] 3.7851288432773873 False {}\n",
      "[-2.99053559e+00  4.90966964e-04] 3.7816655778002426 False {}\n",
      "[-2.99086660e+00 -3.14057615e-04] 3.782567434810958 False {}\n",
      "[-2.99089877e+00  2.06276484e-03] 3.782655015775844 False {}\n",
      "[-2.99216081e+00 -1.47358542e-03] 3.7860789883887644 False {}\n",
      "[-2.99132112e+00  7.94997034e-04] 3.7838033288000683 False {}\n",
      "[-2.98870165e+00 -1.53017553e-03] 3.7766413621249324 False {}\n",
      "[-2.99222667e+00  7.14113989e-04] 3.786257053875751 False {}\n",
      "[-2.99133964e+00  1.48451044e-03] 3.783853637674846 False {}\n",
      "[-2.99265743e+00  3.80996592e-04] 3.787420229892815 False {}\n",
      "[-2.99166456e+00  2.22946159e-04] 3.7847352808790804 False {}\n",
      "[-2.99252587e+00 -8.73675022e-04] 3.787065267640017 False {}\n",
      "[-2.99314441e+00  1.78347064e-03] 3.7887320890996214 False {}\n",
      "[-2.99089832e+00  1.01356266e-03] 3.7826537997169396 False {}\n",
      "[-2.99198253e+00 -1.83344487e-04] 3.7855966458530097 False {}\n",
      "[-2.99096720e+00  3.52156717e-04] 3.782841249489124 False {}\n",
      "[-2.99310830e+00 -1.10900142e-04] 3.7886349149733443 False {}\n",
      "[-2.99215090e+00 -4.63631607e-04] 3.786052176274784 False {}\n",
      "[-2.99035428e+00 -9.60856906e-04] 3.781170942322821 False {}\n",
      "[-2.99099842e+00 -2.64495451e-04] 3.782926167843273 False {}\n",
      "[-2.98984709e+00  2.10449189e-03] 3.7797848407807098 False {}\n",
      "[-2.99176679e+00  2.18474153e-03] 3.7850123839257157 False {}\n",
      "[-2.99082875e+00 -1.63919271e-04] 3.7824644034381985 False {}\n",
      "[-2.99213347e+00  4.15464678e-04] 3.786005047874852 False {}\n",
      "[-2.99129048e+00 -6.34962761e-04] 3.7837201120866806 False {}\n",
      "[-2.99091097e+00  1.51038411e-04] 3.782688206971108 False {}\n",
      "[-2.99085633e+00  7.01780672e-04] 3.782539498352711 False {}\n",
      "[-2.99186543e+00 -2.59107677e-04] 3.7852795913538966 False {}\n",
      "[-2.99453990e+00  2.42702231e-05] 3.7924727977693204 False {}\n",
      "[-2.99176185e+00  1.24412414e-03] 3.7849989877739603 False {}\n",
      "[-2.99008245e+00  2.13842960e-04] 3.780428490798646 False {}\n",
      "[-2.99050709e+00  6.70419377e-05] 3.781587849280468 False {}\n",
      "[-2.99160019e+00 -4.53256204e-04] 3.7845607453385233 False {}\n",
      "[-2.99069644e+00 -6.98053696e-04] 3.782104027125227 False {}\n",
      "[-2.99118658e+00  6.50250038e-05] 3.783437807523414 False {}\n",
      "[-2.99053284e+00 -7.27349812e-04] 3.7816580879353965 False {}\n",
      "[-2.99243192e+00  5.44689778e-04] 3.786811622130525 False {}\n",
      "[-2.99396141e+00  8.29333870e-04] 3.790925454389498 False {}\n",
      "[-2.99121008e+00  5.79975194e-05] 3.783501666657636 False {}\n",
      "[-2.99092283e+00  2.14502416e-03] 3.7827205093411984 False {}\n",
      "[-2.99009391e+00 -6.90043974e-04] 3.780459800331168 False {}\n",
      "[-2.99141941e+00  8.67708164e-04] 3.7840702215420867 False {}\n",
      "[-2.99066554e+00 -6.36594719e-04] 3.782019817173996 False {}\n",
      "[-2.99099736e+00  1.43299860e-04] 3.782923291105134 False {}\n",
      "[-2.99117851e+00  9.10978379e-04] 3.7834158844836 False {}\n",
      "[-2.99105622e+00  7.96449681e-04] 3.7830834056632514 False {}\n",
      "[-2.99275032e+00  1.50927567e-03] 3.7876707310676014 False {}\n",
      "[-2.98967681e+00  1.63145689e-03] 3.779318680287722 False {}\n",
      "[-2.99208080e+00 -1.28323143e-04] 3.7858625881669 False {}\n",
      "[-2.99035130e+00 -1.72491772e-04] 3.7811628037253255 False {}\n",
      "[-2.99178927e+00  1.42883040e-04] 3.785073293064621 False {}\n",
      "[-2.99215641e+00 -2.93715590e-04] 3.7860670994074894 False {}\n",
      "[-2.99123157e+00  4.94039407e-04] 3.7835600597461907 False {}\n",
      "[-2.99280409e+00  2.05996945e-03] 3.787815655530146 False {}\n",
      "[-2.9909142e+00 -3.5069492e-04] 3.7826970075675717 False {}\n",
      "[-2.99090731e+00  1.52000086e-03] 3.7826782557082748 False {}\n",
      "[-2.99145788e+00  6.82542013e-04] 3.784174638904875 False {}\n",
      "[-2.99072630e+00 -1.28274818e-04] 3.782185357020527 False {}\n",
      "[-2.99071790e+00  1.89700152e-03] 3.7821624848573707 False {}\n",
      "[-2.99119929e+00  1.32879722e-03] 3.7834723401701416 False {}\n",
      "[-2.99086639e+00 -1.34845035e-03] 3.7825668817349065 True {'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "show(actor_crit,env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
