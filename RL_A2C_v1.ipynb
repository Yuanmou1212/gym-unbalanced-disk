{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym, gym_unbalanced_disk, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(theta):\n",
    "    return (theta+np.pi)%(2*np.pi) - np.pi # map to [-pi,pi]\n",
    "\n",
    "class Discretize(gym.Wrapper): # only discrete action\n",
    "    def __init__(self,env,num_act=7):\n",
    "        super(Discretize,self).__init__(env) #sets self.env , call function from father calss\n",
    "        self.num_act = num_act\n",
    "        self.action_space = gym.spaces.Discrete(self.num_act)\n",
    "        self.alow, self.ahigh = env.action_space.low, env.action_space.high\n",
    "        # action discrete list.--no need! since we map action from index like to (-3,3)\n",
    "        # self.stepsize = (self.ahigh-self.alow)/self.num_act\n",
    "        # self.act_values_list = np.arange(self.alow,self.ahigh,self.stepsize)\n",
    "        \n",
    "    def step(self,action):\n",
    "        action = self.discretize_act(action)\n",
    "        obs,_,done,info = self.env.step(action)\n",
    "        # velocity from (-infi,infi) to (-pi,pi)\n",
    "        obs[0] = normalize(obs[0])\n",
    "        reward = self.reward_fc(obs,action=action)\n",
    "\n",
    "        return np.array(obs),reward,done,info\n",
    "\n",
    "    def discretize_act(self,action): ##!!! action input is from 0 1 2... num-1;  output is -3,...0,...3\n",
    "        # stepsize = (self.ahigh-self.alow)/self.num_act\n",
    "        # values_list = np.arange(self.alow,self.ahigh,stepsize)\n",
    "        # out = values_list[np.abs(values_list-action).argmin()] \n",
    "        step_size = (self.ahigh-self.alow)/(self.num_act-1)\n",
    "        action = action*step_size +self.alow\n",
    "        return action\n",
    "    \n",
    "    def reward_fc(self,obs,action):\n",
    "        theta = normalize(obs[0]) # already mapped so [-pi,pi]\n",
    "        omega = obs[1]\n",
    "        # reward_vel = omega/40 * np.exp(-abs(theta)) # /40 to reduce -> (0,1) \n",
    "        reward_th =  np.exp(- (abs(theta)-np.pi)**2/(2*(np.pi/10)**2)) # **2 so no abs here!\n",
    "       \n",
    "        if abs(theta)>3 and omega<0.1 :\n",
    "            if abs(action)<=1:\n",
    "                reward =    2*reward_th +5*(3-abs(action)) +2 # 2*reward_vel +10\n",
    "            elif abs(action)>2:\n",
    "                reward =  2*reward_th - 2* abs(action)\n",
    "            else:\n",
    "                reward =    2*reward_th + 1*(3-abs(action)) + 2\n",
    "        elif abs(theta)<1/3*np.pi and omega<0.5:\n",
    "            reward =    2*reward_th + abs(action) -3  # 2*reward_vel -1 \n",
    "        else:\n",
    "            reward =     2*reward_th +2  # 2*reward_vel\n",
    "\n",
    "        # alpha, beta, gamma = 100, 0.05, 0.5\n",
    "        # reward = alpha*theta**2 - beta*omega**2 - gamma*action**2\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def reset(self):\n",
    "        return np.array(self.env.reset())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, env,num_hidden_cri=40, num_hidden_act=40):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        num_inputs = env.observation_space.shape[0] # 2 elements\n",
    "        num_acts = env.action_space.n # discretized action space\n",
    "\n",
    "        # define critic layers \n",
    "        self.cri_linear1 = nn.Linear(num_inputs,num_hidden_cri)\n",
    "        self.cri_linear2 = nn.Linear(num_hidden_cri,1)\n",
    "\n",
    "        # define actor layers\n",
    "        self.act_linear1 =nn.Linear(num_inputs,num_hidden_act)\n",
    "        self.act_linear2 = nn.Linear(num_hidden_act,num_acts)\n",
    "        #self.softmax = nn.Softmax(dim=-1) \n",
    "\n",
    "    def forward(self,state): # (batch, obs)\n",
    "        return self.critic(state),self.actor(state)\n",
    "\n",
    "    def critic(self,state):\n",
    "        a=torch.tanh(self.cri_linear1(state))\n",
    "        out = self.cri_linear2(a)[:,0] # N*1\n",
    "        return out\n",
    "    \n",
    "    def actor(self,state,return_logp=False):\n",
    "        a = torch.tanh(self.act_linear1(state))\n",
    "        a = self.act_linear2(a)\n",
    "        a = a - torch.max(a,dim=1,keepdim=True)[0] # for each sample, find max value action, -max\n",
    "        #p_a = self.softmax(a) # probability\n",
    "\n",
    "        logp = a - torch.log(torch.sum(torch.exp(a),dim=1,keepdim=True)) #log of the softmax, so called log_softmax, is not log(softmax)!\n",
    "\n",
    "        if return_logp ==False:\n",
    "            return torch.exp(logp) # (num_acts,1)\n",
    "        \n",
    "        \n",
    "        if return_logp ==True:\n",
    "\n",
    "            return logp\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROLLOUT interact with env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(actor_crit, env, N_rollout=10_000):\n",
    "    #save the following (use .append)\n",
    "    Start_state = [] # holding an array of (x_t)\n",
    "    Actions = []     # holding an array of (u_t)\n",
    "    Rewards = []     # holding an array of (r_{t+1})\n",
    "    End_state = []   # holding an array of (x_{t+1})\n",
    "    Terminal = []    # holding an array of (terminal_{t+1})\n",
    "    # actor as policy pi\n",
    "    pi = lambda input: actor_crit.actor(torch.tensor(input[None,:],dtype=torch.float32))[0].numpy()\n",
    "    with torch.no_grad():\n",
    "        obs = env.reset()\n",
    "        for i in range(N_rollout):\n",
    "            # based on probability, randomly choose action # based actor results, sample index!?\n",
    "            action = np.random.choice(a=env.action_space.n,p=pi(obs)) #b=) env.act_values_list\n",
    "\n",
    "            Start_state.append(obs)\n",
    "            Actions.append(action)\n",
    "\n",
    "            obs_next, reward, done, info = env.step(action)\n",
    "\n",
    "            terminal = done and not info.get('TimeLimit.truncated', False)\n",
    "\n",
    "            Terminal.append(terminal)\n",
    "            Rewards.append(reward)\n",
    "            End_state.append(obs_next)\n",
    "\n",
    "            if done:\n",
    "                obs = env.reset()\n",
    "            else:\n",
    "                obs = obs_next\n",
    "\n",
    "    #error checking:\n",
    "    assert len(Start_state)==len(Actions)==len(Rewards)==len(End_state)==len(Terminal), f'error in lengths: {len(Start_state)}=={len(Actions)}=={len(Rewards)}=={len(End_state)}=={len(Terminal)}'\n",
    "    return np.array(Start_state), np.array(Actions), np.array(Rewards), np.array(End_state), np.array(Terminal).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_actor(actor_critic, env):\n",
    "    pi = lambda x: actor_critic.actor(torch.tensor(x[None,:],dtype=torch.float32))[0].numpy()\n",
    "    with torch.no_grad():\n",
    "        rewards_acc = 0\n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "            action = np.argmax(pi(obs)) #b=)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            rewards_acc += reward\n",
    "            if done:\n",
    "                return rewards_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A2C_train(actor_critic, optimizer, env, N_iter=21,N_rollout=20000,N_epochs=10, batch_size=32,N_evals=10,\\\n",
    "              alpha_actor=0.5,alpha_entropy=0.5,gamma=0.98):\n",
    "    best = -float('inf')\n",
    "    #torch.save(actor_critic.state_dict(),'A2C-Best.pth')\n",
    "\n",
    "    try:\n",
    "        for iteration in range(N_iter):\n",
    "            print(f'Rollout iteration {iteration+1}')\n",
    "            # rollout to get trajectory record\n",
    "            Start_state, Actions, Rewards, End_state, Terminal = rollout(actor_critic, env, N_rollout=N_rollout)\n",
    "            # data \n",
    "            Start_state = torch.tensor(Start_state,dtype=torch.float32)\n",
    "            Rewards = torch.tensor(Rewards,dtype=torch.float32)\n",
    "            End_state =torch.tensor(End_state,dtype=torch.float32)\n",
    "            Terminal = torch.tensor(Terminal,dtype=torch.float32)\n",
    "            Actions = Actions.astype(int)\n",
    "\n",
    "            print('Starting training on rollout information...')\n",
    "            for epoch in range(N_epochs):\n",
    "                for i in range(batch_size,len(Start_state)+1,batch_size):\n",
    "                    Start_state_batch, Actions_batch, Rewards_batch, End_state_batch, Terminal_batch = \\\n",
    "                    [d[i-batch_size:i] for d in [Start_state, Actions, Rewards, End_state, Terminal]]\n",
    "\n",
    "                    #Advantage:\n",
    "                    Vnow = actor_critic.critic(Start_state_batch) \n",
    "                    Vnext = actor_critic.critic(End_state_batch) \n",
    "                    A = Rewards_batch + gamma*Vnext*(1-Terminal_batch) - Vnow \n",
    "\n",
    "                    # convert from action to index.  # Now action is 0 1 2.. index value，convert in step function=>（-3，3）\n",
    "                    ##### \n",
    "                    \n",
    "                    action_index = np.stack((np.arange(batch_size),Actions_batch),axis=0)\n",
    "                    logp = actor_critic.actor(Start_state_batch,return_logp=True)# return is 【N——batch，num_action】 \n",
    "                    logp_cur = logp[action_index] # do slice，dim0 batch，dim1 use action value ，to get probability\n",
    "                    p = torch.exp(logp) \n",
    "\n",
    "                    L_value_function = torch.mean(A**2) \n",
    "                    L_policy = -(A.detach()*logp_cur).mean() #detach A, the gradient should only to through logp\n",
    "                    L_entropy = -torch.mean((-p*logp),0).sum() \n",
    "\n",
    "                    Loss = L_value_function + alpha_actor*L_policy + alpha_entropy*L_entropy \n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    Loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                print(f'logp{p[0]} logp{logp.shape}')\n",
    "\n",
    "                score = np.mean([eval_actor(actor_critic, env) for i in range(N_evals)])\n",
    "                \n",
    "                print(f'iteration={iteration+1} epoch={epoch+1} Average Reward per episode:',score)\n",
    "                print(f'\\t Value loss:  {L_value_function.item(): .4f}')\n",
    "                print(f'\\t Policy loss: {L_policy.item(): .4f}')\n",
    "                print(f'\\t Entropy:     {-L_entropy.item(): .4f}')\n",
    "\n",
    "                if score>best:\n",
    "                    best = score\n",
    "                    print(f'################################# \\n new best {best: .4f} saving actor-crit... \\n#################################')\n",
    "                    torch.save(actor_critic.state_dict(),'A2C-Best.pth')\n",
    "            print('loading best result')\n",
    "            actor_critic.load_state_dict(torch.load('A2C-Best.pth'))\n",
    "    finally: # this will always run even when using the a KeyBoard Interrupt.\n",
    "        print('loading best result')\n",
    "        actor_critic.load_state_dict(torch.load('A2C-Best.pth'))\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(actor_critic,env):\n",
    "    pi = lambda x: actor_critic.actor(torch.tensor(x[None,:],dtype=torch.float32))[0].numpy()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            obs = env.reset()\n",
    "            env.render()\n",
    "            time.sleep(1)\n",
    "            while True:\n",
    "                action = np.argmax(pi(obs)) \n",
    "                obs, reward, done, info = env.step(action)\n",
    "                print(obs, reward, done, info)\n",
    "                time.sleep(1/60)\n",
    "                env.render()\n",
    "                if done:\n",
    "                    time.sleep(0.5)\n",
    "                    break\n",
    "        finally: #this will always run even when an error occurs\n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5872\\2747579594.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmax_episode_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unbalanced-disk-v0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mumax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_limit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimeLimit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_episode_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_episode_steps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#c)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDiscretize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_act\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "max_episode_steps = 1000 \n",
    "\n",
    "env = gym.make('unbalanced-disk-v0', dt=0.025, umax=3.)\n",
    "env = gym.wrappers.time_limit.TimeLimit(env,max_episode_steps=max_episode_steps) #c)\n",
    "env = Discretize(env, num_act=11)\n",
    "\n",
    "# Define training (Hyper)-parameters\n",
    "gamma = 0.99\n",
    "batch_size = 32 \n",
    "N_iterations = 10\n",
    "N_rollout = 2000#0\n",
    "N_epochs = 5\n",
    "N_evals = 10\n",
    "alpha_actor = 0.5\n",
    "alpha_entropy = 0.6\n",
    "lr = 5e-3\n",
    "\n",
    "\n",
    "assert isinstance(env.action_space,gym.spaces.Discrete), 'action space requires to be discrete'\n",
    "\n",
    "actor_crit = ActorCritic(env, num_hidden_act=40,num_hidden_cri=40)\n",
    "optimizer = torch.optim.Adam(actor_crit.parameters(), lr=lr) #low learning rate\n",
    "\n",
    "A2C_train(actor_crit, optimizer, env, N_iter=N_iterations,N_rollout=N_rollout,N_epochs=N_epochs, batch_size=batch_size ,N_evals=N_evals,\\\n",
    "              alpha_actor=alpha_actor,alpha_entropy=alpha_entropy,gamma=gamma)\n",
    "\n",
    "plt.plot([eval_actor(actor_crit, env) for i in range(100)],'.')\n",
    "plt.show()\n",
    "show(actor_crit,env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22952\\351026519.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_crit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'show' is not defined"
     ]
    }
   ],
   "source": [
    "show(actor_crit,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def _init_(self, env, num_hidden=40):\n",
    "        super(Critic, self)._init_()\n",
    "        num_inputs = env.observation_space.shape[0] + env.action_space.shape[0]\n",
    "\n",
    "        self.cri_linear1 = nn.Linear(num_inputs,num_hidden)\n",
    "        self.cri_linear2 = nn.Linear(num_hidden,1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        h = torch.tanh(self.cri_linear1(x))\n",
    "        return self.cri_linear2(h)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def _init_(self, env, num_hidden=40):\n",
    "        super(Actor, self)._init_()\n",
    "        num_inputs = env.observation_space.shape[0]\n",
    "\n",
    "        self.act_linear1 = nn.Linear(num_inputs,num_hidden)\n",
    "        self.act_linear2 = nn.Linear(num_hidden,env.action_space.shape[0])\n",
    "\n",
    "    def forward(self, state):\n",
    "        h = torch.tanh(self.act_linear1(state))\n",
    "        return torch.tanh(self.act_linear2(h))  # Assume the action space is -1 to 1\n",
    "\n",
    "def train(env, actor, critic, num_episodes=1000):\n",
    "    optimizer_actor = torch.optim.Adam(actor.parameters())\n",
    "    optimizer_critic = torch.optim.Adam(critic.parameters())\n",
    "    targets = [0, np.radians(10), np.radians(-10)]  # Targets are upright and ±10°\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = actor(torch.tensor(state, dtype=torch.float32))\n",
    "            next_state, reward, done, _ = env.step(action.detach().numpy())\n",
    "            \n",
    "            # Update critic\n",
    "            q_value = critic(torch.tensor(state, dtype=torch.float32), action)\n",
    "            with torch.no_grad():\n",
    "                next_action = actor(torch.tensor(next_state, dtype=torch.float32))\n",
    "                next_q_value = critic(torch.tensor(next_state, dtype=torch.float32), next_action)\n",
    "                expected_q_value = reward + 0.99 * next_q_value * (1 - done)\n",
    "            loss_critic = (q_value - expected_q_value).pow(2)\n",
    "            optimizer_critic.zero_grad()\n",
    "            loss_critic.backward()\n",
    "            optimizer_critic.step()\n",
    "            \n",
    "            # Update actor\n",
    "            policy_loss = -critic(torch.tensor(state, dtype=torch.float32), actor(torch.tensor(state, dtype=torch.float32))).mean()\n",
    "            optimizer_actor.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            optimizer_actor.step()\n",
    "\n",
    "            state = next_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
